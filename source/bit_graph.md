---
title: 位图
---

# 位图
## 爬虫 URL 去重
爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。搜索引擎的爬虫系统会爬取几十亿、上百亿的
网页。如何实现 url 去重，避免重复爬取网页。

思路很简单：记录已经爬取的 url。

## 解析
要记录已经爬取的 url，那么使用什么数据结构来存储。这里涉及到两个操作，添加和查询 url。

散列表，红黑树，跳表都可以实现快速的插入和查找。

### 散列表
1. 内存空间占用大
使用散列表存储，如果爬取 10 亿个 url，假设一个 url 长度为 64 bytes，那么存储 10 亿个 url 至少需要大约 60 GB 的内存。
而且散列表要维持较小的装载因子，以免过多出现散列冲突，导致性能下降。而且如果用链表法解决冲突，还要存储链表指针。
所以散列表存储 10 亿 url 需要内存远大于 60 GB，甚至超过 100 GB。

2. 查询耗时
如果基于链表法解决冲突，散列表中存储的是 URL，那当查询的时候，通过哈希函数定位到某个链表之后，还需要依次比对每个链表中
的 URL。链表中的结点在内存中不是连续存储的，所以不能一下子加载到 CPU 缓存中，没法很好地利用到 CPU 高速缓存，所以数据访
问性能方面会打折扣。

3. 字符串匹配耗时
链表中的每个数据都是 URL，而 URL 平均长度为 64 字节的字符串，要让待判重的 URL，跟链表中的每个 URL，做字符串匹配。比较耗时。

### 位图
