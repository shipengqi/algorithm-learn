'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/algorithm-learn/docs/18_ac/','title':"AC 自动机",'content':"BF 算法、RK 算法、BM 算法、KMP 算法，都是单模式串匹配算法。Trie 树是多模式串匹配算法。\n 单模式串匹配算法，就是在一个主串中查找一个模式串。 多模式串匹配算法，就是在一个主串中查找多个模式串。  AC 自动机（Aho-Corasick）算法，是基于 Trie 树的一种改进算法，它跟 Trie 树的关系，就像单模式串中，KMP 算法与 BF 算法的关系一样。\n如何实现敏感词过滤功能\r#\r\r可以针对每个敏感词，通过单模式串匹配算法（比如 KMP 算法）与用户输入的文字内容进行匹配。但是，这样每个匹配过程都需要扫描一遍用户输入的内容。 整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，假如有上千个字符，那就需要扫描几千遍这样的 输入内容。很显然，这种处理思路比较低效。\n多模式匹配算法要更高效。如何用 Trie 树实现敏感词过滤功能？\n把敏感词字典构建成 Trie 树结构。用户输入的内容作为主串，从第一个字符（假设是字符 C）开始，在 Trie 树中匹配。当匹配到 Trie 树的叶子节点， 或者中途遇到不匹配字符的时候，将主串的开始匹配位置后移一位，也就是从字符 C 的下一个字符开始，重新在 Trie 树中匹配。 这种处理方法，有点类似单模式串匹配的 BF 算法。可以借鉴 KMP 算法对多模式串 Trie 树进行改进。这就需要 AC 自动机算法。\nAC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上。\n"});index.add({'id':1,'href':'/algorithm-learn/docs/','title':"Docs",'content':""});index.add({'id':2,'href':'/algorithm-learn/docs/17_trie/','title':"Trie 树",'content':"Trie 树\r#\r\rTrie 树（字典树）。树形结构。是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n例如，一个字符串集合中 6 个字符串： how，hi，her，hello，so，see。如果要在里面多次查找某个字符串是否存在。 如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，效率肯定是比较低的。如何高效查询？\n利用 Trie 树结构，利用字符串之间的公共前缀，将重复的前缀合并在一起。如：\n根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串 （红色节点并不都是叶子节点）。\n构造 Trie 树结构的过程：\n当在 Trie 树中查找一个字符串的时候，比如 “her”，将要查找的字符串分割成单个的字符 h，e，r，然后从 Trie 树的根节点开始 匹配。依次找到 h，e，r 三个节点。\n如果要查找 “he”，从根节点开始，找到 h，e 节点。但是，路径的最后一个节点 “e” 并不是红色的。也就是说，“he” 是某个字 符串的前缀子串，但并不能完全匹配任何字符串。\n实现一棵 Trie 树\r#\r\r二叉树中，一个节点的左右子节点是通过两个指针来存储的，但是 Trie 树是一个多叉树，如何存储子节点？使用数组。 借助散列表的思想，通过一个下标与字符一一映射的数组，来存储子节点的指针。\n假设字符串中只有从 a 到 z 这 26 个小写字母，在数组中下标为 0 的位置，存储指向子节点 a 的指针，下标为 1 的位置存储指向子节点 b 的指针， 以此类推，下标为 25 的位置，存储的是指向的子节点 z 的指针。如果某个字符的子节点不存在，就在对应的下标的位置存储 null。\n在 Trie 树中查找字符串的时候，就可以通过字符的 ASCII 码减去 a 的 ASCII 码，迅速找到匹配的子节点的指针。比如，d 的 ASCII 码减去 a 的 ASCII 码就是 3，那子节点 d 的指针就存储在数组中下标为 3 的位置中。\ntype TrieNode struct { data string isEndingChar bool children []*TrieNode } func NewTrieNode(data string) *TrieNode { return \u0026amp;TrieNode{data, false, make([]*TrieNode, 26)} } type TrieTree struct { root *TrieNode } func NewTrieTree() *TrieTree { return \u0026amp;TrieTree{NewTrieNode(\u0026#34;/\u0026#34;)} } func (t *TrieTree) Insert(data string) { var basicChar rune = \u0026#39;a\u0026#39; node := t.root for _, s := range data { index := int(s) - int(basicChar) if node.children[index] == nil { node.children[index] = NewTrieNode(string(s)) } node = node.children[index] } node.isEndingChar = true } func (t *TrieTree) Find(pattern string) bool { var basicChar rune = \u0026#39;a\u0026#39; node := t.root for _, s := range pattern { index := int(s) - int(basicChar) if node.children[index] == nil { return false } node = node.children[index] } if !node.isEndingChar { // 只匹配到部分前缀  return false } return true } Trie 的时间复杂度\r#\r\r构建 Trie 树的过程，需要扫描所有的字符串，时间复杂度是 O(n)，n 表示所有字符串的长度和。\n如果要查询的字符串长度是 k，只需要比对大约 k 个节点，就能完成查询操作。所以，构建好 Trie 树后，查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度。\nTrie 占用的内存\r#\r\rTrie 树用的是一种空间换时间的思路，实现 Trie 的时候，用数组来存储一个节点的子节点的指针。如果字符串中包含从 a 到 z 这 26 个字符，那每个节 点都要存储一个长度为 26 的数组，并且每个数组存储一个 8 字节指针（或者是 4 字节，这个大小跟 CPU、操作系统、编译器等有关）。而且，即便一个 节点只有很少的子节点，远小于 26 个，比如 3、4 个，也要维护一个长度为 26 的数组。\nTrie 树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符的存储远远大于 1 个字节。按照上面例子，数组长度为 26，每个元素 是 8 字节，那每个节点就会额外需要 26*8=208 个字节。而且这还是只包含 26 个字符的情况。如果字符串中不仅包含小写字母，还包含大写字母、数字、 甚至是中文，那需要的存储空间就更多了。\n也就是说，在某些情况下，Trie 树不一定会节省存储空间。在重复的前缀并不多的情况下，Trie 树不但不能节省内存，还有可能会浪费更多的内存。\n可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。比如有序数组、跳表、散列表、红黑树等。\nTrie 与散列表、红黑树\r#\r\r散列表、红黑树、跳表等这些数据结构也可以实现在一组字符串中查找字符串的功能。跟 Trie 树比较一下，看看它们各自的优缺点和应用场景。\nTrie 树它对要处理的字符串要求：\n 字符串中包含的字符集不能太大。如果字符集太大，那存储空间可能就会浪费很多。 要求字符串的前缀重合比较多，不然空间消耗会变大很多。 通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好。  Trie 树不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串。比如索引擎的搜索关键词提示 功能。当你在搜索引擎的搜索框中，输入要搜索的文字的某一部分的时候，搜索引擎就会自动弹出下拉框，里面是各种关键词提示。\n"});index.add({'id':3,'href':'/algorithm-learn/docs/09_binary_search/','title':"二分查找",'content':"二分查找\r#\r\r二分查找是一种非常简单易懂的快速查找算法，比如说，一个猜数字游戏。猜一个 0 到 99 之间的数字，猜的过程中，每猜一次，就会告诉你猜的大了还是 小了，直到猜中为止。如何快速猜中数字？\n这个例子用的就是二分思想，按照这个思想 7 次就猜出来了。\n二分查找的时间复杂度\r#\r\r假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。\n其中 n/2^k=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度 就是 O(k)。通过 n/2^k=1，我们可以求得 k=log2(n)，所以时间复杂度就是 O(logn)。\nO(logn) 这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1)的算法还要高效。\n因为 logn 是一个非常“恐怖”的数量级，即便 n 非常非常大，对应的 logn 也很小。比如 n 等于 2 的 32 次方，这个数很大了吧？大约是 42 亿。 也就是说，如果我们在 42 亿个数据中用二分查找一个数据，最多需要比较 32 次。\n用大 O 标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1) 有可能表示的是一个非常大的常量值，比 如 O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高。\n实现\r#\r\rfunc BinarySearch(a []int, value int) int { length := len(a) if length \u0026lt; 1 { return -1 } start := 0 end := length -1 for start \u0026lt;= end { mid := (start + end) / 2 if a[mid] == value { return mid } else if a[mid] \u0026lt; value { start = mid + 1 } else { end = mid - 1 } } return -1 } // 使用递归实现二分查找 func BinarySearchRecursive(a []int, value int) int { length := len(a) if length \u0026lt; 1 { return -1 } return bs(a, value, 0, length -1) } func bs(a []int, value, start, end int) int { if start \u0026gt; end { return -1 } mid := (start + end) / 2 if a[mid] == value { return mid } else if a[mid] \u0026lt; value { return bs(a, value, mid + 1, end) } else { return bs(a, value, start, mid - 1) } } 上面的代码中，mid := (start + end) / 2 这种写法是有问题的。因为如果 start 和 end 比较大的话，两者之和就有可能会溢出。改进的 方法是将 mid 的计算方式写成 start+(end-start)/2。更进一步，如果要将性能优化到极致的话，可以将这里的除以 2 操作转化成位 运算start+((end-start)\u0026gt;\u0026gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。\n应用场景\r#\r\r二分查找的应用场景是有很大局限性：\n 二分查找依赖的是顺序表结构，也就是数组。不能依赖链表的主要原因是二分查找算法需要按照下标随机访问元素。数组按照下标随机访问数据 的时间复杂度是 O(1)，而链表随机访问的时间复杂度是 O(n)。所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。 二分查找针对的是有序数据。如果数据没有序，需要先排序。排序的时间复杂度最低是 O(nlogn)。所以，如果我们针对的是一组静态的数据， 没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。如果我们的数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除 操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都是很高的。 数据量太小不适合二分查找。顺序遍历就足够了。 数据量太大也不适合二分查找。二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要 求比较苛刻。比如，1GB 大小的数据，如果用数组来存储，那就需要 1GB 的连续内存空间。  查找第一个值等于给定值的元素\r#\r\r前面实现的二分查找，适用于有序数据集合中不存在重复的数据的情况，如果有重复数据，如何找到第一个等于给定值的元素？\nfunc BinarySearch(a []int, value int) int { length := len(a) if length \u0026lt; 1 { return -1 } start := 0 end := length -1 for start \u0026lt;= end { mid := (start + end) / 2 if a[mid] \u0026gt; value { end = mid - 1 } else if a[mid] \u0026lt; value { start = mid + 1 } else { if mid == 0 || a[mid - 1] != value { return mid } else { end = mid - 1 } } } return -1 } 注意 a[mid] == value 时，要判断否有重复值，数据集合是有序的，要找到第一个等于给定值得元素：\n 如果 mid == 0 说明，已经是 a[mid] 集合的第一个元素，不需要再继续查找。 如果 a[mid - 1] != value，也就是 a[mid] 的前一个元素不等于 value，那么就说明 a[mid] 就是第一个等于给定值得元素。  查找最后一个值等于给定值的元素\r#\r\rfunc BinarySearch(a []int, value int) int { length := len(a) if length \u0026lt; 1 { return -1 } start := 0 end := length -1 for start \u0026lt;= end { mid := (start + end) / 2 if a[mid] \u0026gt; value { end = mid - 1 } else if a[mid] \u0026lt; value { start = mid + 1 } else { if mid == length - 1 || a[mid + 1] != value { return mid } else { start = mid + 1 } } } return -1 } 和查找最后一个值等于给定值的元素思路差不多，注意 a[mid] == value 时：\n 如果 mid == length - 1 说明，已经是 a[mid] 集合的最后一个元素，不需要再继续查找。 如果 a[mid + 1] != value，也就是 a[mid] 的下一个元素不等于 value，那么就说明 a[mid] 就是最后一个等于给定值得元素。  查找第一个大于等于给定值的元素\r#\r\rfunc BinarySearch(a []int, value int) int { length := len(a) if length \u0026lt; 1 { return -1 } start := 0 end := length -1 for start \u0026lt;= end { mid := (start + end) / 2 if a[mid] \u0026gt;= value { if mid == 0 || a[mid - 1] \u0026lt; value { return mid } else { end = mid - 1 } } else { start = mid + 1 } } return -1 } 如果 a[mid] 大于等于给定值 value ，先看这个 a[mid] 是不是要找的第一个值大于等于给定值的元素。\n 如果 mid == 0 说明，已经是 a[mid] 集合的第一个元素，不需要再继续查找。 如果 a[mid - 1] \u0026lt; value，也就是前面一个元素小于要查找的值 value，那么就说明 a[mid] 就是我们要找得元素。  查找最后一个小于等于给定值的元素\r#\r\rfunc BinarySearch(a []int, value int) int { length := len(a) if length \u0026lt; 1 { return -1 } start := 0 end := length -1 for start \u0026lt;= end { mid := (start + end) / 2 if a[mid] \u0026lt;= value { if mid == length - 1 || a[mid + 1] \u0026gt; value { return mid } else { start = mid + 1 } } else { end = mid - 1 } } return -1 } 如果 a[mid] 小于等于给定值 value ，先看这个 a[mid] 是不是要找的第一个值小于等于给定值的元素。\n 如果 mid == length - 1 说明，已经是 a[mid] 集合的最后一个元素，不需要再继续查找。 如果 a[mid + 1] \u0026gt; value，后面一个元素大于要查找的值 value，那么就说明 a[mid] 就是我们要找得元素。  "});index.add({'id':4,'href':'/algorithm-learn/docs/23_bit_map/','title':"位图",'content':"爬虫 URL 去重\r#\r\r爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。搜索引擎的爬虫系统会爬取几十亿、上百亿的 网页。如何实现 url 去重，避免重复爬取网页。\n思路很简单：记录已经爬取的 url。\n解析\r#\r\r要记录已经爬取的 url，那么使用什么数据结构来存储。这里涉及到两个操作，添加和查询 url。\n散列表，红黑树，跳表都可以实现快速的插入和查找。\n散列表\r#\r\r  内存空间占用大 使用散列表存储，如果爬取 10 亿个 url，假设一个 url 长度为 64 bytes，那么存储 10 亿个 url 至少需要大约 60 GB 的内存。 而且散列表要维持较小的装载因子，以免过多出现散列冲突，导致性能下降。而且如果用链表法解决冲突，还要存储链表指针。 所以散列表存储 10 亿 url 需要内存远大于 60 GB，甚至超过 100 GB。\n  查询耗时 如果基于链表法解决冲突，散列表中存储的是 URL，那当查询的时候，通过哈希函数定位到某个链表之后，还需要依次比对每个链表中 的 URL。链表中的结点在内存中不是连续存储的，所以不能一下子加载到 CPU 缓存中，没法很好地利用到 CPU 高速缓存，所以数据访 问性能方面会打折扣。\n  字符串匹配耗时 链表中的每个数据都是 URL，而 URL 平均长度为 64 字节的字符串，要让待判重的 URL，跟链表中的每个 URL，做字符串匹配。比较耗时。\n  位图\r#\r\r如果有 1 千万个整数，整数的范围在 1 到 1 亿之间。如何快速查找某个整数是否在这 1 千万个整数中？ 申请一个大小为 1 亿、数据类型为布尔类型（true 或者 false）的数组。将这 1 千万个整数作为数组下标，将对应的数组值设置成 true。 比如，整数 5 对应下标为 5 的数组值设置为 true，也就是 array[5] = true。\n当查询某个整数 K 是否在这 1 千万个整数中的时候，只需要将对应的数组值 array[K] 取出来，看是否等于 true。如果等于 true， 那说明 1 千万整数中包含这个整数 K；相反，就表示不包含这个整数 K。\n大部分语言的布尔类型，大小是 1 个字节的，并不能节省太多内存空间。实际上，表示 true 和 false 两个值，只需要用一个二进 制位（bit）就可以了。\n可以借助编程语言中提供的数据类型，比如 int、long、char 等类型，通过位运算，用其中的某个位表示某个数字。\n// BitMap implement bitmap type BitMap []byte // New create BitMap func New(length uint) BitMap { return make([]byte, length/8+1) } // Set set value in bitmap func (b BitMap) Set(value uint) { byteIndex := value / 8 if byteIndex \u0026gt;= uint(len(b)) { return } bitIndex := value % 8 []byte(b)[byteIndex] |= 1 \u0026lt;\u0026lt; bitIndex } // Get check whether value exist or not func (b BitMap) Get(value uint) bool { byteIndex := value / 8 if byteIndex \u0026gt;= uint(len(b)) { return false } bitIndex := value % 8 return []byte(b)[byteIndex]\u0026amp;(1\u0026lt;\u0026lt;bitIndex) != 0 } 如果用散列表存储这 1 千万的数据，数据是 32 位的整型数，也就是需要 4 个字节的存储空间，那总共至少需要 40MB 的存储空间。如果通过位图 的话，数字范围在 1 到 1 亿之间，只需要 1 亿个二进制位，也就是 12MB 左右的存储空间就够了。\n使用位图的问题\r#\r\r如果数字的范围很大，比如刚刚那个问题，数字范围是 1 到 10 亿，那位图的大小就是 10 亿个二进制位，也就是 120MB 的大小，消耗的内存空间， 不降反增。\n布隆过滤器\r#\r\r布隆过滤器是基于位图改进的，就是为了解决位图内存消耗的问题。\n还是数据个数是 1 千万，数据的范围是 1 到 10 亿。布隆过滤器的做法是，仍然使用一个 1 亿个二进制大小的位图，然后通过哈希函数，对数 字进行处理，让它落在这 1 到 1 亿范围内。\n哈希冲突问题\r#\r\r哈希函数会存在冲突的问题，布隆过滤器的处理方法。既然一个哈希函数可能会存在冲突，那用多个哈希函数一块儿定位一个数据。 使用 K 个哈希函数，对同一个数字进行求哈希值，那会得到 K 个不同的哈希值，我们分别记作 X1，X2，X3，…，XK。把这 K 个数字作为位图中的下 标，将对应的 BitMap[X1]，BitMap[X2]，BitMap[X3]，…，BitMap[XK]都设置成 true，也就是说，用 K 个二进制位，来表示一个 数字的存在。\n查询某个数字是否存在的时候，用同样的 K 个哈希函数，对这个数字求哈希值，看这 K 个哈希值，对应位图中的数值是否都为 true，如果都是 true， 则说明，这个数字存在，如果有其中任意一个不为 true，那就说明这个数字不存在。\n误判问题\r#\r\r使用多个哈希函数的处理方式，容易发生误判的问题。\n如果某个数字经过布隆过滤器判断不存在，那说明这个数字真的不存在；如果某个数字经过布隆过滤器判断存在，这个时候才会有可能误判，有可 能并不存在。\n总结\r#\r\r虽然布隆过滤器会存在误判，但是，这并不影响它发挥大作用。很多场景对误判有一定的容忍度。比如爬虫判重这个问题，即便一个没有被爬取过的网页， 被误判为已经被爬取，对于搜索引擎来说，也并不是什么大事情，是可以容忍的，毕竟网页太多了，搜索引擎也不可能 100% 都爬取到。\n假设需要判重的网页有 10 亿，可以用一个 10 倍大小的位图来存储，也就是 100 亿个二进制位，换算成字节，那就是大约 1.2GB。如果用散列表判 重，需要至少 100GB 的空间。布隆过滤器在存储空间的消耗上，降低了非常多。\n布隆过滤器用多个哈希函数对同一个网页链接进行处理，只需要将网页链接从内存中读取一次，进行多次哈希计算，理论上讲这组操作是 CPU 密集 型的。而在散列表的处理方式中，需要读取散列冲突拉链的多个网页链接，分别跟待判重的网页链接，进行字符串匹配。这个操作涉及很多内存数据 的读取，所以是内存密集型的。CPU 计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。\n"});index.add({'id':5,'href':'/algorithm-learn/docs/20_deivide_conquer/','title':"分治算法",'content':"理解分治算法\r#\r\r分治算法（divide and conquer）的核心思想其实就是四个字，分而治之，也就是将原问题划分成 n 个规模较小，并且结构与原问题 相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n分治有点类似递归的定义。区别是分治算法是一种处理问题的思想，递归是一种编程技巧。 分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题  分治算法能解决的问题，一般需要满足下面这几个条件：\n 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果。  "});index.add({'id':6,'href':'/algorithm-learn/docs/22_dynamic_programming/','title':"动态规划",'content':"动态规划\r#\r\r"});index.add({'id':7,'href':'/algorithm-learn/docs/25_vector/','title':"向量空间",'content':"如何实现一个简单的音乐推荐系统？\n核心思想：\n 找到口味偏好相似的用户，推荐他们爱听的歌曲； 找出跟你喜爱的歌曲特征相似的歌曲，推荐这些歌曲  基于相似用户做推荐\r#\r\r如下图，把听类似歌曲的人，看做口味相似的用户。用 “1” 表示“喜爱”，用 “0” 表示“不发表意见”。从图中可以看出，你跟小明共同喜爱的歌曲最多， 有 5 首。可以得出，小明跟你的口味非常相似。\n可以通过用户的行为，来定义对歌曲的喜爱程度。给每个行为定义一个得分，得分越高表示喜爱程度越高。\n这里统计两个用户之间的相似度，就需要使用另外一个距离，那就是欧几里得距离（Euclidean distance）。欧几里得距离是用来计算两个向量 之间的距离的。\n一维空间是一条线，用 1，2，3 … 这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，用（1，3）（4，2）（2，2）… 这样的两个数， 来表示二维空间中的某个位置；三维空间是一个立体空间，用（1，3，5）（3，1，7）（2，4，3）… 这样的三个数，来表示三维空间中的某个位置。 K 维空间中的某个位置，可以写作 （$X_{1}$，$X_{2}$，$X_{3}$，…，$X_{K}$）。这种表示方法就是向量（vector）。\n如何计算两个向量之间的距离\r#\r\r类比到二维、三维空间中距离的计算方法，得到两个向量之间距离的计算公式：\n把每个用户对所有歌曲的喜爱程度，都用一个向量表示。计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量：\n结论是，小明跟你的口味最相似。\n基于相似歌曲做推荐\r#\r\r如何判断两首歌曲是否相似？\n对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，给每个歌曲的 每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。\n"});index.add({'id':8,'href':'/algorithm-learn/docs/12_hash_algo/','title':"哈希算法",'content':"哈希算法\r#\r\r不管是“散列”还是“哈希”，英文都是 “Hash”。\n哈希算法的定义和原理，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数 据映射之后得到的二进制值串就是哈希值。\n实现哈希算法的要求：\n 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）； 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同； 散列冲突的概率要很小； 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。  应用\r#\r\r安全加密\r#\r\r哈希算法最先想到的应该就是安全加密。最常用于加密的哈希算法是 MD5（MD5 Message-Digest Algorithm，MD5消息摘要算法）和 SHA（Secure Hash Algorithm，安全散列算法）。\n不管是什么哈希算法，只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说？\n鸽巢原理（也叫抽屉原理）：它是说，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个。\n哈希算法产生的哈希值的长度是固定且有限的。比如 MD5 算法的哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据， 而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。一般情况下， 哈希值越长的哈希算法，散列冲突的概率越低。\n唯一标识\r#\r\r如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名 称不同图片内容相同的情况。该如何搜索？\n比较笨的办法就是，拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是图片越大，就越耗时。\n更好的方法是给每一个图片取一个唯一标识，或者说信息摘要。比如，可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再 取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判 定图片是否在图库中，这样就可以减少很多工作量。\n数据校验\r#\r\rBT 下载的原理是基于 P2P 协议的。我们从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每 块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。\n网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果没有能力 检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。\n可以通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变， 最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的 哈希值比对。如果不同，说明这个文件块不完整或者被篡改了。\n散列函数\r#\r\r散列函数也是哈希算法的一种应用。\n负载均衡\r#\r\r负载均衡算法有很多，比如轮询、随机、加权轮询等。如何在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上？\n可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由 到的服务器编号。\n数据分片\r#\r\r假设现在图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台 机器的内存上限。我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。每次从图库中读取 一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列 表。当要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就 去编号 k 的机器构建的散列表中查找。\n分布式存储\r#\r\r如果有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。\n如何决定将哪个数据放到哪个机器上？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值 就是应该存储的缓存机器编号。\n但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这时候麻烦就来了。因为，这里并不是简单地 加个机器就可以了。原来的数据是通过与 10 来取模的。比如 13 这个数据，存储在编号为 3 这台机器上。但是新加了一台机器中，我们对数据按 照 11 取模，原来 13 这个数据就被分配到 2 号这台机器上了。\n因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存， 直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n假设我们有 k 个机器，数据的哈希值的范围是 [0, MAX]。我们将整个范围划分成 m 个小区间（ m 远大于 k），每个机器负责 m/k 个小区间。当有 新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数 量的均衡。\n"});index.add({'id':9,'href':'/algorithm-learn/docs/21_backtracking/','title':"回溯算法",'content':"回溯算法\r#\r\r"});index.add({'id':10,'href':'/algorithm-learn/docs/15_graph/','title':"图",'content':"图\r#\r\r图也是一种非线性表数据结构，比树更复杂。涉及图的算法有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二 分图等等。\n树中的元素称为节点，图中的元素叫作顶点（vertex）。\n图中的一个顶点可以与任意其他顶点建立连接关系。这种建立的关系叫作边（edge）。\n生活中就有很多符合图这种结构的例子。比如，社交网络，就是一个非常典型的图结构。拿微信举例子。可以把每个用户看作一个顶点。如果两个用户之间互加 好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。每个用户有多少个好友，对应到图中，就叫作顶点的度（degree）， 就是跟顶点相连接的边的条数。\n微博的社交关系比微信更复杂一点。微博允许单向关注，也就是说，用户 A 关注了用户 B，但用户 B 可以不关注用户 A。就要引入边的“方向”的概念。\n这种边有方向的图叫作有向图。边没有方向的图就叫作无向图。\n无向图中有“度”这个概念，表示一个顶点有多少条边。在有向图中，把度分为入度（In-degree）和出度（Out-degree）。\n顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。\nQQ 中的社交关系要还更复杂。QQ 不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经 常往来，亲密度就比较低。\n这里就要用到另一种图，带权图（weighted graph）。在带权图中，每条边都有一个权重（weight），可以通过这个权重来表示 QQ 好友 间的亲密度。\n图的存储\r#\r\r邻接矩阵\r#\r\r邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，就将 A[i][j] 和 A[j][i] 标记为 1； 对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那就将 A[i][j] 标记为 1。同理，如果有一条箭 头从顶点 j 指向顶点 i 的边，就将 A[j][i] 标记为 1。对于带权图，数组中就存储相应的权重。\n邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。\n如果存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好几亿的 用户，对应到图上就是好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。\n邻接表\r#\r\r邻接表有点像散列表，每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。图中画的是一个有向图的邻接表存储方式，每个顶点对应的 链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点。\n邻接表存储起来比较节省空间，但是使用起来就比较耗时间。\n比如上图中，如果要确定，是否存在一条从顶点 2 到顶点 4 的边，就要遍历顶点 2 对应的那条链表，看链表中是否存在顶点 4。而且，链表的存储方式 对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。\n在基于链表法解决冲突的散列表中，如果链过长，为了提高查找效率，可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。邻接表长得很像散列。 所以，也可以将邻接表同散列表一样进行“改进升级”。\n可以将邻接表中的链表改成平衡二叉查找树。实际开发中，可以选择用红黑树。这样，就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉 查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两 个顶点之间否是存在边。\n逆邻接表\r#\r\r用一个邻接表来存储这种有向图有时候是不够的。比如微博中去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就 是用户的粉丝列表，是非常困难的。\n需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。\n深度和广度优先搜索\r#\r\r在社交网络中，有一个六度分割理论，具体是说，你与世界上的另一个人间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。 一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友的好友。在社交网络中，往往通过用户之 间的连接关系，来实现推荐“可能认识的人”这么一个功能。\n如何找出一个用户的所有三度（其中包含一度、二度和三度）好友关系？\n广度优先搜索\r#\r\r广度优先搜索（Breadth-First-Search），简称为 BFS。其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的， 依次往外搜索。\n// s 起始顶点，t 终止顶点 func (g *Graph) BFS(s, t int) { if s == t { return } // init prev 记录搜索路径 \tprev := make([]int, g.v) for index := range prev { prev[index] = -1 } // search by queue \tvar queue []int // 一个队列，存储已经被访问、但相连的顶点还没有被访问的顶点 \tvisited := make([]bool, g.v) // 记录已经被访问的顶点，避免顶点被重复访问 \tqueue = append(queue, s) visited[s] = true // 顶点设置为 true 表示已经被访问 \tisFound := false for len(queue) \u0026gt; 0 \u0026amp;\u0026amp; !isFound { top := queue[0] queue = queue[1:] linkedList := g.adj[top] for e := linkedList.Front(); e != nil; e = e.Next() { k := e.Value.(int) if !visited[k] { prev[k] = top if k == t { isFound = true break } queue = append(queue, k) visited[k] = true } } } if isFound { printPrev(prev, s, t) } else { fmt.Printf(\u0026#34;no path found from %d to %d\\n\u0026#34;, s, t) } } prev 用来记录搜索路径。当从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。不过，这个路径是反向存储的。 prev[w] 存储的是，顶点 w 是从哪个前驱顶点遍历过来的。比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3] 就等于 2。为了正 向打印出路径，需要递归地来打印 printPrev。\n广度优先搜索的时间、空间复杂度\r#\r\r最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以， 广度优先搜索的时间复杂度是 O(V+E)，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都 是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 O(E)。\n广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数， 所以空间复杂度是 O(V)。\n深度优先搜索\r#\r\r深度优先搜索（Depth-First-Search），简称 DFS。 假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走， 直到最终找到出口。这种走法就是一种深度优先搜索策略。\n图中寻找一条从顶点 s 到顶点 t 的路径，s 就可以理解为迷宫中起始的位置，t 代表出口。\n深度优先搜索找到的并不是顶点 s 到顶点 t 的最短路径。\nfunc (g *Graph) DSF(s, t int) { if s == t { return } // init prev 记录搜索路径 \tprev := make([]int, g.v) for index := range prev { prev[index] = -1 } visited := make([]bool, g.v) // 记录已经被访问的顶点，避免顶点被重复访问 \tvisited[s] = true // 顶点设置为 true 表示已经被访问  g.recurse(s, t, prev, visited, false) printPrev(prev, s, t) } func (g *Graph) recurse(s int, t int, prev []int, visited []bool, isFound bool) { if isFound { return } visited[s] = true if s == t { isFound = true return } linkedList := g.adj[s] for e := linkedList.Front(); e != nil; e = e.Next() { k := e.Value.(int) if !visited[k] { prev[k] = s g.recurse(k, t, prev, visited, false) } } } 深度度优先搜索的时、空间间复杂度\r#\r\r从图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。\n深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最 大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。\n"});index.add({'id':11,'href':'/algorithm-learn/docs/14_heap/','title':"堆",'content':"堆\r#\r\r堆是一种特殊的树。\n 堆必须是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右 子节点的值。  对于每个节点的值都大于等于子树中每个节点值的堆，叫作大顶堆。对于每个节点的值都小于等于子树中每个节点值的堆，叫作小顶堆。\n将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。\n堆最经典的应用就是堆排序了。\n实现一个堆\r#\r\r完全二叉树比较适合用数组来存储，非常节省存储空间。\n用数组存储堆的例子：\n数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储 的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。\n插入\r#\r\r往堆中插入一个元素后，需要继续满足堆的两个特性。如果把新插入的元素放到堆的最后，如下图，已经不符合堆的特性。于是， 就需要进行调整，让其重新满足堆的特性，这个过程叫作堆化（heapify）。\n堆化有两种，从下往上和从上往下。先看从下往上的堆化方法。\n堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。\n让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那 种大小关系。\ntype Heap struct { a []int // 数组，从下标 1 开始存储 \tn int // 堆可以存储的元素的最大个数 \tcount int // 堆已经存储的元素的个数 } // top-max heap -\u0026gt; heapify from down to up func (h *Heap) insert(data int) { if h.count \u0026gt;= h.n { // 堆满了 \treturn } h.count ++ h.a[h.count] = data // compare with parent node \ti := h.count parent := i / 2 // fmt.Println(i, parent) \tfor parent \u0026gt; 0 \u0026amp;\u0026amp; h.a[parent] \u0026lt; h.a[i] { swap(h.a, parent, i) i = parent parent = i / 2 } } func swap(a []int, i, j int) { a[i], a[j] = a[j], a[i] } 删除堆顶元素\r#\r\r堆顶元素存储的就是堆中数据的最大值或者最小值。\n假设构造的是大顶堆，堆顶元素就是最大的元素。当删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。 然后再迭代地删除第二大节点，以此类推，直到叶子节点被删除。\n上面的方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性。\n把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点 之间满足大小关系为止。这就是从上往下的堆化方法。\n一个包含 n 个节点的完全二叉树，树的高度不会超过 log2n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比， 也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。\nfunc (h *Heap) removeMax() { if h.count == 0 { // 堆空了 \treturn } // swap max and last \tswap(h.a, 1, h.count) // heapify from up to down \theapifyUpToDown(h.a, h.count) } func heapifyUpToDown(a []int, count int) { for i := 1; i \u0026lt;= count/2; { maxIndex := i if a[i] \u0026lt; a[i * 2] { maxIndex = i * 2 } if i * 2 + 1 \u0026lt;= count \u0026amp;\u0026amp; a[maxIndex] \u0026lt; a[i * 2 + 1] { maxIndex = i * 2 + 1 } if maxIndex == i { break } swap(a, i, maxIndex) i = maxIndex } } 堆排序\r#\r\r堆排序是一种原地的、时间复杂度为 O(nlogn) 的排序算法。\n堆排序的过程大致分解成两个大的步骤，建堆和排序。\n建堆\r#\r\r所谓“原地”就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路：\n 在堆中插入一个元素的思路。尽管数组中包含 n 个数据，但是可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，调用前面讲的插入 操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。 第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且 每个数据都是从上往下堆化。  // build a heap func buildHeap(a []int, n int) { // heapify from the last parent node \tfor i := n / 2; i \u0026gt;= 1; i-- { // 对下标从 n/2 开始到 1 的数据进行堆化，因为叶子节点堆化比较的是自己，所以叶子节点不需要堆化 \theapifyUpToDown(a, i, n) // 从第一个非叶子节点 n/2 开始堆化 \t} } 排序\r#\r\r建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。把它跟最后一个元素交换，那最大元素 就放到了下标为 n 的位置。\n这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素（堆顶是最大的）移除之后，把下标为 n 的元素放到堆顶，然后再通过堆化 的方法，将剩下的 n-1 个元素重新构建成堆。堆化完成之后，再取堆顶的元素，放到下标是 n-1 的位置，一直重复这个过程，直到最后堆中只剩下标 为 1 的一个元素，排序工作就完成了。\n// sort by ascend, a index begin from 1, has n elements func sort(a []int, n int) { buildHeap(a, n) k := n for k \u0026gt;= 1 { swap(a, 1, k) heapifyUpToDown2(a, 1, k-1) k-- } } 快速排序和堆排序\r#\r\r实际开发中，为什么快速排序要比堆排序性能好？\n 堆排序数据访问的方式没有快速排序友好。 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。  堆的应用\r#\r\r优先级队列\r#\r\r优先级队列，首先它是一个队列。队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来， 优先级最高的，最先出队。\n用堆来实现优先级队列是最直接、最高效的。因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。往优先级队列中插入 一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。\n优先级队列的使用场景：\n合并有序小文件\r#\r\r假设有100个小文件，每个文件的大小是 100 MB，每个文件中存储的都是有序的字符串。希望将这些 100 个小文件合并成一个有序的大文件。 这里就会用到优先级队列。\n整体思路有点像归并排序中的合并函数。从这 100 个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合 并后的大文件中，并从数组中删除。\n假设，这个最小的字符串来自于 13.txt 这个小文件，就再从这个小文件取下一个字符串，并且放到数 组中，重新比较大小，并且选择最小的放入合并后的大文件，并且将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大 文件为止。\n这里用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然， 这不是很高效。有没有更加高效方法呢？这里就可以用到优先级队列，也可以说是堆。将从小文件中取出来的字符串放入到小顶堆中， 那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。将这个字符串放入到大文件中，并将其从堆中删除。然后再从 小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。\n高性能定时器\r#\r\r假设一个定时器，维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描 一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。\n但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效，主要原因有两点：\n 任务的约定执行时间离当前时间可能还有很久，前面很多次扫描其实都是徒劳的； 每次都要扫描整个任务列表，如果任务列表很大的话，会比较耗时。  针对这样的问题，就可以用优先级队列来解决。按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶） 存储的是最先执行的任务。\n这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。这个 时间间隔 T 就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在 T 秒之后，再来执行任 务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。\n当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为 定时器执行下一个任务需要等待的时间。这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。\n求 Top K\r#\r\r求 Top K 的问题可以抽象成两类：\n 针对静态数据集合，也就是说数据集合事先确定，不会再变。 针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。  针对静态数据，如何在一个包含 n 个数据的数组中，查找前 K 大数据呢？我们可以维护一个大小为 K 的小顶堆，顺序遍历数组，从 数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不 做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。\n遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，所以时间 复杂度就是 O(nlogK)。\n针对动态数据求得 Top K 就是实时 Top K。举一个例子。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 K 大数据。\n如果每次询问前 K 大数据，都基于当前的数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。实际上，可以一 直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，就拿它与堆顶的元素对比。如果比堆顶元素大，就把堆顶元素删除，并且将这 个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，都可以里立刻返回给他。\n求中位数\r#\r\r中位数，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第 n/2+1 个数据就是中位数；如果数据的个 数是偶数的话，那处于中间位置的数据有两个，第 n/2 个和第 n/2+1 个数据，这个时候，可以随意取一个作为中位数，比如取两 个数中靠前的那个，就是第 n/2 个数据。\n对于一组静态数据，中位数是固定的，我们可以先排序，第 n/2 个数据就是中位数。\n但是，如果面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序， 那效率就不高了。\n借助堆这种数据结构，不用排序，就可以非常高效地实现求中位数操作。\n需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的 数据。\n也就是说，如果有 n 个数据，n 是偶数，从小到大排序，那前 n/2 个数据存储在大顶堆中，后 n/2 个数据存储在小顶堆中。这样， 大顶堆中的堆顶元素就是要找的中位数。\n如果 n 是奇数，情况是类似的，大顶堆就存储 n/2 + 1 个数据，小顶堆中就存储 n/2 个数据。\n数据是动态变化的，当新添加一个数据的时候，如何调整两个堆，让大顶堆中的堆顶元素继续是中位数？\n如果新加入的数据小于等于大顶堆的堆顶元素，就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，就将这个 新数据插入到小顶堆。\n这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果 n 是偶数，两个堆中的数据个数都是 n/2；如果 n 是奇数， 大顶堆有 n/2 + 1 个数据，小顶堆有 n/2 个数据。这个时候，可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整， 来让两个堆中的数据满足上面的约定。\n于是，就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间 复杂度变成了 O(logn)，但是求中位数只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 O(1)。\n如何快速求接口的 99% 响应时间\r#\r\r中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面 50% 的数据。99 百分位数的概念可以类 比中位数，如果将一组数据从小到大排列，这个 99 百分位数就是大于前面 99% 数据的那个数据。\n维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 n*99% 个数据，小顶堆中保存 n*1% 个数据。 大顶堆堆顶的数据就是要找的 99% 响应时间。\n插入一个数据也类似中位数的操作。\n通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。每次求 99% 响应时间的时候，直接 返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。\n"});index.add({'id':12,'href':'/algorithm-learn/docs/01_complex_analysis/','title':"复杂度分析",'content':"为什么需要复杂度分析\r#\r\r在实际工作中，我们把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析？\n首先这种方法叫事后统计法，这种方法的局限性：\n 依赖测试环境。代码在不同的环境运行，结果是不同的，比如一个酷睿 i9，和酷睿 i3，很明显 i9 处理速度要快的多。 测试结构受数据规模的影响。测试数据规模太小，测试结果可能无法真实地反应算法的性能。  大 O 时间复杂度表示法\r#\r\rfunc cal(n int) int { sum := 0 for i := 1; i \u0026lt;= n; ++i { sum = sum + i } return sum } 上面的代码，假设每行代码执行的时间都一样，为 unit_time。那么，第 2、3 行代码分别需要 1 个 unit_time 的执行时间， 第 4、5 行都运行了 n 遍，所以需要 2n * unit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)* unit_time。 可以看出来，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。\nfunc cal2(n int) int { sum := 0 for i := 1; i \u0026lt;= n; i++ { for j := 1; j \u0026lt;= n; j ++ { sum = sum + i } } return sum } 上面的代码，第 2、3 行代码，每行都需要 1 个 unit_time 的执行时间，第 4 行代码循环执行了 n 遍，需要 n * unit_time的执 行时间，第 5,6 行代码循环执行了 n^2 遍，所以需要 2n^2 * unit_time 的执行时间。所以，整段代码总的执行 时间T(n) = (2n^2+n+3)*unit_time。\n我们不知道 unit_time 的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是， 所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。 把这个规律总结成一个公式:\nT(n) = O(f(n))\rT(n) 表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。公式中的 O，表示代码的执行 时间 T(n) 与 f(n) 表达式成正比。\n所以，第一个例子中的 T(n) = O(2n+2)，第二个例子中的 T(n) = O(2n^2+n+3)。这就是大 O 时间复杂度表示法。\n大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时 间复杂度（asymptotic time complexity），简称时间复杂度。\n如何分析一段代码的时间复杂度\r#\r\r 只关注循环执行次数最多的一段代码，例如，第一个例子中的 T(n) = O(2n+2)，总的时间复杂度就是 O(n)。第二个例子中 的 T(n) = O(2n^2+n+3)。总的时间复杂度就是 O(n^2)。 加法法则：总复杂度等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积  func cal3(n int) int { ret := 0 for i := 1; i \u0026lt;= n; i++ { ret = ret + f(i) } return ret } func f(n int) int { sum := 0 for i := 1; i \u0026lt;= n; i++ { sum = sum + i } return sum } 单独看 cal3() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。 但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是， T(n) = T1(n) * T2(n) = O(n*n) = O(n^2)。\n复杂度量级\r#\r\r按数量级递增：\n常量阶 O(1) 指数阶 O(2^n)\r对数阶 O(logn) 阶乘阶 O(n!)\r线性阶 O(n)\r线性对数阶 O(nlogn)\r平方阶 O(n^2)，立方阶 O(n^3)，k 次方阶 O(n^k)\r上面的复杂度量级，可以分为两类，多项式量级和非多项式量级。\n非多项式量级\r#\r\r时间复杂度为非多项式量级的算法问题叫作 NP（Non-Deterministic Polynomial，非确定多项式）问题。\n非多项式量级只有两个：O(2^n) 和 O(n!)。\n当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复 杂度的算法其实是非常低效的算法。\n常量阶 O(1)\r#\r\rO(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有 3 行，它的时间复杂度 也是 O(1)，而不是 O(3)。\ni := 8; j := 6; sum := i + j; 只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循 环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是 Ο(1)。\n对数阶 O(logn) 线性对数阶 O(nlogn)\r#\r\r对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。\ni := 1; for i \u0026lt;= n { i = i * 2; } 上面的代码中，变量 i 的值从 1 开始取，每循环一次就乘以 2。当大于 n 时，循环结束。变量 i 的取值就是一个等比数列。 我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 2^x=n 求解 x。x=log2(n)，所以，这段代码的时间复杂度 就是 O(log2(n))。\n不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。\n如果理解了 O(logn)，那 O(nlogn) 就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是 O(logn)， 我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速 排序的时间复杂度都是 O(nlogn)。\n空间复杂度分析\r#\r\r空间复杂度全称是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。\nfunc DemopPint(n int) { a := make([]int, n) for i := 0; i \u0026lt; n; i ++ { a = append(a, i * i) } for i := n - 1; i \u0026gt;= 0; i -- { fmt.Println(a[i]) } } 第 2 行代码中，申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复 杂度就是 O(n)。\n常见的空间复杂度就是 O(1)、O(n)、O(n^2)，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。\n最好、最坏情况时间复杂度\r#\r\r// n 表示数组 array 的长度 int func find(array []int, n, x int) int { pos := -1 for i := 0; i \u0026lt; n; i ++ { if array[i] == x { pos = i } } return pos } 按照前面的方法分析，很明显复杂度是 O(n)。\n但是在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。我们优化一下代码：\nfunc find(array []int, n, x int) int { pos := -1 for i := 0; i \u0026lt; n; i ++ { if array[i] == x { pos = i break } } return pos } 优化以后复杂度明显不再是 O(n)。\n为了表示代码在不同情况下的不同时间复杂度，需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度。\n最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。比如上面的例子，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。复杂度就是 O(1)。\n同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。上面的例子，如果数组中没有要查找的变量 x，我们需 要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。复杂度就是O(n)。\n平均情况时间复杂度\r#\r\r最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度， 需要引入另一个概念：平均情况时间复杂度，后面我简称为平均时间复杂度。\n还是前面的例子，要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0~n-1 位置中和不在数组中。我们把每种情况下，查找 需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值。时间复杂度的大 O 标记法中，可以省略掉系数、 低阶、常量，所以，咱们把刚刚这个公式简化之后，得到的平均时间复杂度就是 O(n)。\n(1 + 2 + 3 + n + n)/(n + 1) = n(n + 3)/2(n + 1)\r这个结论虽然是正确的，但是计算过程稍微有点儿问题。是什么问题？ n+1 种情况，出现的概率并不是一样的。\n要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，假设在数组中与不在数组中的概率 都为 1/2。另外，要查找的数据出现在 0~n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的 数据出现在 0~n-1 中任意位置的概率就是 1/(2n)。\n引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍 然是 O(n)。\n很多时候，我们使用一个复杂度就可以满足需求了。\n均摊时间复杂度\r#\r\r均摊时间复杂度就是一种特殊的平均时间复杂度，一般均摊时间复杂度就等于最好情况时间复杂度。\n"});index.add({'id':13,'href':'/algorithm-learn/docs/16_string_match_algo/','title':"字符串匹配算法",'content':"字符串匹配算法\r#\r\r字符串匹配算法很常用，比如 js 中的 indexOf 函数，就依赖字符串匹配算法。\nBF 算法\r#\r\rBF （Brute Force）算法叫作暴力匹配算法，也叫朴素匹配算法。简单，但是性能差。\n字符串匹配算法有两个概念：主串和模式串。比如两个字符串 A 和 B，要在 A 中查找 B，A 就是主串，B 就是模式串。 主串的长度记作 n，模式串的长度记作 m。\nBF 算法的思想就是在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。\n比如主串是 \u0026ldquo;aaaaa…aaaaaa\u0026rdquo;（省略号表示有很多重复的字符 a），模式串是 \u0026ldquo;aaaaab\u0026rdquo;。每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的 最坏情况时间复杂度是 O(n*m)。\n实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。朴素的字符串匹配算法就够用了。\nRK 算法\r#\r\rRK 算法（Rabin-Karp）其实就是 BF 算法的升级版。BF 算法需要暴力地对比这 n-m+1 个子串与模式串，但是，每次检查主串与子串是否匹配，需要依 次比对每个字符，所以 BF 算法的时间复杂度就比较高，是 O(n*m)。\nRK 算法的思想是通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等（如果不 考虑哈希冲突的问题），那就说明对应的子串和模式串匹配了。但这只是提高了模式串与子串比较的效率。\n提高哈希算法计算子串哈希值的效率\r#\r\r怎样设计哈希算法，假设要匹配的字符串的字符集中只包含 K 个字符，可以用一个 K 进制数来表示一个子串，这个 K 进制数转化成十进制数，作为子 串的哈希值。\n比如要处理的字符串只包含 a～z 这 26 个小写字母，那就用二十六进制来表示一个字符串。把 a～z 这 26 个字符映射到 0～25 这 26 个数 字，a 就表示 0，b 就表示 1，以此类推，z 表示 25。\nRK 算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有 子串的哈希值了，所以这部分的时间复杂度是 O(n)。\n模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 O(1)，总共需要比较 n-m+1 个子串的哈希值，所以，这部分的时间复杂度也是 O(n)。 所以，RK 算法整体的时间复杂度就是 O(n)。\n当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。\n解决方法很简单。当发现一个子串的哈希值跟模式串的哈希值相等的时候，只需要再对比一下子串和模式串本身就好了。\n哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致 RK 算法的时间复杂度退化，效率下降。\nBM 算法\r#\r\r上面的例子，模式串中不包含 c，所以，模式串向后滑动时，只要 c 与模式串有重合，肯定不匹配。所以，把模式串往后多滑动几位，把模式串移动 到 c 的后面。\n当遇到不匹配的字符时，有什么固定的规律，往后多滑动几位？BM 算法，就是在寻找这种规律。\nBM 算法包含两部分：\n 坏字符规则（bad character rule） 好后缀规则（good suffix shift）。  坏字符规则\r#\r\rBM 算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。\n从模式串的末尾往前倒着匹配，当发现某个字符没法匹配的时候。把这个没有匹配的字符叫作坏字符（主串中的字符），上面的 c 就是是坏字符。 并且 c 并不在模式串中，那么可以直接将模式串移动到 c 的后面，再继续比较。这个时候模式串 \u0026ldquo;abd\u0026rdquo; 和 \u0026ldquo;aca\u0026rdquo; 比较， a 就是坏字符，但是 模式串中下标是 0 的位置是字符 a，只需要往后滑动两位，两个 a 对齐，再继续比较。\n如何寻找滑动的规律？\n发生不匹配的时，把坏字符对应的模式串中的字符下标记作 si。如果坏字符在模式串中存在，把这个坏字符在模式串中的下标记作 xi。如果不存在，把 xi 记作 -1。那模式串往后移动的位数就等于 si-xi。（注意，这里的下标，都是字符在模式串的下标）。如果坏字符在模式串里多处出现，在计 算 xi 的时候，选择最靠后的那个位置的下标。 避免可能匹配的情况被滑动略过。\nBM 算法在最好情况下的时间复杂度非常低，是 O(n/m)。比如，主串是 \u0026ldquo;aaabaaabaaabaaab\u0026rdquo;，模式串是 \u0026ldquo;aaaa\u0026rdquo;。每次比对，模式串都可以直 接后移四位。\n单纯使用坏字符规则还是不够的。因为根据 si-xi 计算出来的移动位数，有可能是负数，比如主串是 \u0026ldquo;aaaaaaaaaaaaaaaa\u0026rdquo;，模式串是 \u0026ldquo;baaa\u0026rdquo;。不但 不会向后滑动模式串，还有可能倒退。\n好后缀规则\r#\r\r好后缀规则跟坏字符规则的思路类似。\n已经匹配的 bc 叫作好后缀，记作 {u}。拿它在模式串中查找，如果找到了另一个跟 {u} 相匹配的子串 {u*}，那就将模式串滑动 到子串 {u*} 与主串中 {u} 对齐的位置。\n如果在模式串中找不到另一个等于 {u} 的子串，就直接将模式串，滑动到主串中 {u} 的后面。但是有时候不能直接滑动到 {u} 的后面，比如：\n针对这种情况，不仅要看好后缀在模式串中，是否有另一个匹配的子串，还要检查好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。\nBM 算法实现\r#\r\r当模式串和主串中的某个字符不匹配的时候，选择坏字符规则还是好后缀规则？\n分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免前面提到的，根据坏字符规则， 计算得到的往后滑动的位数，有可能是负数的情况。\n遇到坏字符时，要计算往后移动的位数 si-xi，如何求得 xi？如果在模式串中顺序遍历查找，就会比较低效。更加高效的方式是使用散列表。\n将模式串中的每个字符及其下标都存到散列表中。这样就可以快速找到坏字符在模式串的位置下标了。\n实现一种最简单的情况，假设字符串的字符集不是很大，每个字符长度是 1 字节，用大小为 256 的数组，来记录每个字符在模式串中出现的位置。数组的 下标对应字符的 ASCII 码值，数组中存储这个字符在模式串中出现的位置。\nfunc generateBC(pattern string) []int { bc := make([]int, 256) for index := range bc { bc[index] = -1 } for index, char := range pattern { bc[int(char)] = index // int(char) 计算 char 的 ASCII 值 \t} return bc } func bm(main string, pattern string) int { bc := generateBC(pattern) } KMP 算法\r#\r\rKMP （Knuth Morris Pratt）算法核心思想和 BM 算法类似。区别是 KMP 算法的两部分是：\n 坏字符 好前缀  坏字符仍然是不能匹配的那个字符。已经匹配的那段字符串叫作好前缀。\n"});index.add({'id':14,'href':'/algorithm-learn/docs/07_sort/','title':"排序",'content':"排序\r#\r\r常用的三类排序算法：\n   算法 时间复杂度     冒泡、插入、选择 O(n^2)   快排、归并 O(nlogn)   桶、计数、基数 O(n)    插入排序和冒泡排序的时间复杂度相同，为什么更倾向于使用插入排序算法而不是冒泡排序算法？\n分析排序算法\r#\r\r执行效率\r#\r\r排序算法执行效率的分析，可以通过几个方面来衡量：\n 最好情况、最坏情况、平均情况时间复杂度 时间复杂度的系数、常数、低阶我们知道，时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但 是实际的软件开发中，排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，就要把 系数、常数、低阶也考虑进来。 基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果在分析排序算法的执行效率的时候， 应该把比较次数和交换（或移动）次数也考虑进去。  内存消耗\r#\r\r算法的内存消耗可以通过空间复杂度来衡量，针对排序算法的空间复杂度，有一个概念，原地排序（Sorted in place），就是特指“空间复杂度” 是 O(1) 的排序算法。冒泡排序，插入排序，选择排序都是原地排序算法。\n稳定性\r#\r\r稳定性就是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。\n比如一组数据 2，9，3，4，8，3，按照大小排序之后就是 2，3，3，4，8，9。\n这组数据里有两个 3。经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法； 如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。\n为什么要考察排序算法的稳定性\r#\r\r真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序。 比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望 按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。\n最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思 路理解起来不难，但是实现起来会很复杂。\n借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后， 我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。\n为什么呢？稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排 序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。\n冒泡排序\r#\r\r冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡 会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。\nfunc BubbleSort(a []int) { length := len(a) if len(a) \u0026lt;= 1 { return } for i := 0; i \u0026lt; length; i ++ { flag := false for j := 0; j \u0026lt; length - i - 1; j ++ { if a[j] \u0026gt; a[j + 1] { a[j], a[j + 1] = a[j + 1], a[j] flag = true } } if !flag { break } } } 分析冒泡排序\r#\r\r 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。当有相邻的两个元素大小相等的时候，不做交换，相同大小的数据在排序 前后不会改变顺序，所以冒泡排序是稳定的排序算法。 最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最 坏的情况是，要排序的数据刚好是倒序排列的，需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n^2)。  插入排序\r#\r\r插入排序，将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序 区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次 比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位。\nfunc InsertSort(a []int) { n := len(a) if n \u0026lt;= 1 { return } for i := 1; i \u0026lt; n; i++ { value := a[i] j := i - 1 // 查找要插入的位置并移动数据 \tfor ; j \u0026gt;= 0; j-- { if a[j] \u0026gt; value { a[j+1] = a[j] } else { break } } a[j+1] = value } } 分析插入排序\r#\r\r 插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是一个原地排序算法。 插入排序中，对于值相同的元素，可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所 以插入排序是稳定的排序算法。 如果要排序的数据已经是有序的，并不需要搬移任何数据。如果从尾到头在有序数据组里面查找插入位置，每次只需要比较一个 数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n)。注意，这里是从尾到头遍历已经有序的数据。如果数组 是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n^2)。  选择排序\r#\r\r选择排序有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\nfunc SelectionSort(a []int) { length := len(a) if len(a) \u0026lt;= 1 { return } for i := 0; i \u0026lt; length; i ++ { minIndex := i for j := i + 1; j \u0026lt; length; j ++ { // 找到最小值  if a[j] \u0026lt; a[minIndex] { minIndex = j } } a[i], a[minIndex] = a[minIndex], a[i] // 交换 \t} } 分析选择排序\r#\r\r 选择排序算法的运行并不需要额外的存储空间，空间复杂度是 O(1)，是一个原地排序算法。 选择排序中，每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n^2)。因为每一轮选择排序都要遍历未排序区间，找到最小值。  为什么插入排序要比冒泡排序更受欢迎\r#\r\r冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个。\nif a[j] \u0026gt; a[j + 1] { a[j], a[j + 1] = a[j + 1], a[j] flag = true } if a[j] \u0026gt; value { a[j + 1] = a[j] } else { break } 归并排序\r#\r\r归并排序使用的就是分治思想。分治，就是将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。\n分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧。\nfunc MergeSort(a []int) { length := len(a) if len(a) \u0026lt;= 1 { return } mergeSort(a, 0, length - 1) } func mergeSort(arr []int, start, end int) { if start \u0026gt;= end { return } mid := (start + end) / 2 mergeSort(arr, start, mid) mergeSort(arr, mid + 1, end) merge(arr, start, mid, end) } // arr 是原切片，只传入分组后的下标 func merge(arr []int, start, mid, end int) { // 申请一个可以存放两个分组所有元素的临时切片 \ttmpArr := make([]int, end - start + 1) i := start // 第一个分组的开始位置 \tj := mid + 1 // 第二个分组的开始位置 \tk := 0 for ; i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= end; k++ { // 每个分组都已经排好顺序 \tif arr[i] \u0026lt; arr[j] { // 直接比较每个分组相同位置的元素大小 \ttmpArr[k] = arr[i] // 小的元素放到临时切片中 \ti++ } else { tmpArr[k] = arr[j] j++ } } copy(arr[start : end + 1], tmpArr) // 将临时切片的元素拷贝到原切片的对应位置 } 分析归并排序\r#\r\r 因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。归并排序不是原地排序算法。每次合并操作都需要申 请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空 间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。 合并的过程中，如果 A[p…q] 和 A[q+1…r] 之间有值相同的元素，我们可以先把 A[p…q] 中的元素放入 tmp 数组。这样就保证了 值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法 归并排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(nlogn)。  快速排序\r#\r\r快速排序简称“快排”，使用的也是分治思想。\n快排的思想：如果要排序数组中下标从 p 到 r 之间的一组数据，选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。我们遍历 p 到 r 之间的 数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分， 前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。\nfunc QuickSort(a []int) { length := len(a) if len(a) \u0026lt;= 1 { return } separateSort(a, 0, length - 1) } func separateSort(arr []int, start, end int) { if start \u0026gt;= end { return } i := partition(arr, start, end) separateSort(arr, start, i-1) separateSort(arr, i+1, end) } 如果我们不考虑空间消耗的话，partition() 分区函数可以写得非常简单。我们申请两个临时数组 X 和 Y，遍历 A[p…r]，将小于 pivot 的元素 都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。但是，这样就不是原地排 序算法了。\nfunc partition(arr []int, start, end int) int { // 选取最后一位当对比数字 \tpivot := arr[end] var i = start for j := start; j \u0026lt; end; j++ { if arr[j] \u0026lt; pivot { if !(i == j) { // 交换位置 \tarr[i], arr[j] = arr[j], arr[i] } i++ } } arr[i], arr[end] = arr[end], arr[i] return i } 处理有点类似选择排序。通过游标 i 把 A[p…r-1] 分成两部分。A[p…i-1] 的元素都是小于 pivot 的，暂且叫它“已处理区间”，A[i…r-1] 是“未处理区间”。每次都从未处理的区间 A[i…r-1] 中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾 部，也就是 A[i] 的位置。\n快排和归并的区别\r#\r\r 归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。 归并排序非原地排序算法，快速排序可以实现原地排序。 归并排序是稳定的排序，快速排序不是。  "});index.add({'id':15,'href':'/algorithm-learn/docs/11_hash_table/','title':"散列表",'content':"散列表\r#\r\r散列表（Hash Table），也叫哈希表或者 Hash 表。\n散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。\n通过散列函数把元素的键值映射为数组下标，然后将数据存储在数组中对应下标的位置。当按照键值查询元素时，用同样的散列函数，将键 值转化数组下标，从对应的数组下标的位置取数据。\n散列函數\r#\r\r散列函数在散列表中起着非常关键的作用。把它定义成 hash(key)，key 表示元素的键值，hash(key) 计算得到一个散列值。\n散列函数设计的基本要求：\n 散列函数计算得到的散列值是一个非负整，因为数组下标是从 0 开始的。 如果 key1 = key2，那么 hash(key1) == hash(key2) 如果 key1 != key2，那么 hash(key1) != hash(key2)  第三点要注意，在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像的 MD5、SHA、CRC 等哈希算法， 也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。\n散列冲突\r#\r\r再好的散列函数也无法避免散列冲突。那该如何解决散列冲突问题？\n常用的散列冲突解决方法有两类：\n 开放寻址法（open addressing） 链表法（chaining）  开放寻址法\r#\r\r开放寻址法的思想是，如果出现了散列冲突，就重新探测一个空闲位置，将其插入。一种比较简单的探测方法，线性探测（Linear Probing）：\n当往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，就从当前位置开始，依次往后查找，看是否有空闲位置， 直到找到为止。\n如下图，黄色的色块表示空闲位置，橙色的色块表示已经存储了数据： 图中散列表的大小为 10，在元素 x 插入散列表之前，散列表中已有 6 个元素。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是 这个位置已经有数据了，所以就产生了冲突。于是就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是再从表头 开始找，直到找到空闲位置 2，于是将其插入到这个位置。\n在散列表中查找元素的过程有点儿类似插入过程。通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的 元素。如果相等，则说明就是要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。\n删除操作\r#\r\r散列表跟数组一样，还支持删除操作。对于使用线性探测法解决冲突的散列表，注意删除操作不能单纯地把要删除的元素设置为空。因为查找 操作遍历到数组中的空闲位置，还没有找到，就认为数据不存在。但是，如果这个空闲位置是后来删除的，就会导致原来的查找算法失效。存在的数据， 会被认定为不存在。如何解决？\n可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。\n线性探测法的问题\r#\r\r当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下， 可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要 查找或者删除的数据。\n二次探测\r#\r\r二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2 …… 而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2，hash(key)+2^2……\n双重散列\r#\r\r意思就是不仅要使用一个散列函数。使用一组散列函数 hash1(key)，hash2(key)，hash3(key)…… 先用第一个散列函数，如果计 算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。\n装载因子\r#\r\r不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，会尽可能保证 散列表中有一定比例的空闲槽位。用装载因子（load factor）来表示空位的多少。\n装载因子的计算公式：散列表的装载因子=填入表中的元素个数/散列表的长度\n装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n链表法\r#\r\r链表法是一种更加常用的散列冲突解决办法。如下图，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素 都放到相同槽位对应的链表中。 插入的时候，只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时， 同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。\n时间复杂度\r#\r\r查找或删除操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散 列中数据的个数，m 表示散列表中“槽”的个数。\n设计散列表\r#\r\r散列表的查询效率并不能笼统地说成是 O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高， 都可能导致散列冲突发生的概率升高，查询效率下降。\n如果有恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果使用的是基于链表的冲突解决 方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从 O(1) 退化为 O(n)。\n如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直接点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需 要 1 万秒。这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就 是散列表碰撞攻击的基本原理。\n设计散列函数\r#\r\r散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。\n 设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能。 散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均， 不会出现某个槽内数据特别多的情况。 其他因素，长度、特点、分布、散列表的大小等。  装载因子过大\r#\r\r装载因子越大，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。\n 对于没有频繁插入和删除的静态数据集合来说，很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为之前数据都是已知的。 对于动态散列表来说，数据集合是频繁变动的，事先无法预估将要加入的数据个数，所以也无法事先申请一个足够大的散列表。随着数据慢慢加入，装 载因子就会慢慢变大。当装载因子大到一定程度之后，可以进行动态扩容。  动态扩容\r#\r\r散列表的动态扩容，可以重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容都申请一个原来散列表大小两倍的空间。如果原来 散列表的装载因子是 0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了 0.4。\n针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以需要通过散列函数重新计算每个数据的存储 位置。\n图中，散列表中 21 这个元素原来存储在下标为 0 的位置，搬移到新的散列表中，存储在下标为 7 的位置。\n当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高， 可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值。\n避免低效地扩容\r#\r\r动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢， 甚至会无法接受。\n如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时 候，“一次性”扩容的机制就不合适了。\n为了解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并 不将老的数据搬移到新散列表中。\n有新数据要插入时，将新数据插入新散列表中，并且从老的散列表中拿出一些数据放入到新散列表。每次插入一个数据到散列表，都重复上面的过程。 经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。\n选择冲突解决方法\r#\r\r散列冲突的解决办法，开放寻址法和链表法。 Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。\n开放寻址法\r#\r\r开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表， 序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。\n用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比 起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n当数据量比较小、装载因子小的时候，适合采用开放寻址法。\n链表法\r#\r\r链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是链表优于 数组的地方。\n链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量 的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找 效率有所下降，但是比起顺序查找还是快很多。\n链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的， 不是连续的，所以对 CPU 缓存是不友好的，这方面对于执行效率也有一定的影响。\n如果存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4 个字节或者 8 个字节），那链表中指针的内存消耗在大对象面前就可以 忽略了。\n基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替 链表*。\n散列表举例分析\r#\r\rJava 的 HashMap 是一个工业级的散列表。\n初始大小\r#\r\rHashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数， 这样会大大提高 HashMap 的性能。\n装载因子和动态扩容\r#\r\r最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容 为原来的两倍大小。\n散列冲突解决方法\r#\r\rHashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重 影响 HashMap 的性能。\n于是，在 JDK1.8 版本中，为了对 HashMap 做进一步优化，引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。 当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。\n散列函数\r#\r\r散列函数的设计并不复杂，追求的是简单高效、分布均匀。\nint hash(Object key) { int h = key.hashCode()； return (h ^ (h \u0026gt;\u0026gt;\u0026gt; 16)) \u0026amp; (capitity -1); //capicity表示散列表的大小 } 散列表和链表\r#\r\rRedis 有序集合不仅使用了跳表，还用到了散列表。Java 的 LinkedHashMap 也用到了散列表和链表。为什么散列表和链表会经常放到一块使用？\n散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法 支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那需要将散列表中的数据拷贝到数组中，然后排序，再遍历。\n为了解决这个问题，将散列表和链表（或者跳表）结合在一起使用。\nLRU 缓存淘汰算法\r#\r\r之前使用单链表可以实现 LRU 缓存淘汰算法，但是时间复杂度是 O(n)。借助散列表可以实现时间复杂度是 O(1) 的 LRU 算法。\n一个缓存（cache）系统主要包含下面这几个操作：\n 往缓存中添加一个数据； 从缓存中删除一个数据； 在缓存中查找一个数据。 这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 O(n)。如果将散列表和链表两种数据结构组合使用，可以将这三个操作 的时间复杂度都降低到 O(1)。具体的结构就是下面这个样子：  使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。\n因为散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚提到的双向链表，另一个链是散列表中的拉链。前驱和后继指 针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。\n前面讲到的缓存的三个操作，是如何做到时间复杂度是 O(1) 的：\n 散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，可以很快地在缓存中找到一个数据。当找到数据之后，还需要将它移动到双向链表的尾部。 借助散列表，可以在 O(1) 时间复杂度里找到要删除的结点。因为是双向链表，所以可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在 双向链表中，删除结点只需要 O(1) 的时间复杂度。 添加数据到缓存稍微有点麻烦，需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有 没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。  "});index.add({'id':16,'href':'/algorithm-learn/docs/02_array/','title':"数组",'content':"数组\r#\r\r几乎每种语言都有数组这种数据类型，为什么数组的下标都是从 0 开始？\n数组的随机访问\r#\r\r数组是一种线性表的数据结构，用一组连续的内存空间，来存储一组相同类型的数据。\n线性表\n线性表就是数据排程一条线一样的结构。线性表上的数据只有前后两个方向。（链表，栈，队列也是线性表结构）。\n二叉树，图，堆等是非线性表结构。非线性表中数据不再是简单的前后关系。\n连续的内存空间和相同类型的数据\n连续的内存空间和相同类型的数据，正式因为这两个限制，才使数组可以实现随机访问。但是这两个限制也让其他的 操作变得效率低下，比如在数组中插入或删除一个值，为了保证数据的连续，就必须进行数据搬移。\n例如：一个长度为 10 的 int 类型的数组 a := new([10]int)。计算机给数组 a[10]，分配了一块连续内存空间 1000～1039， 其中，内存块的首地址为 base_address = 1000。\n计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通 过下面的寻址公式，计算出该元素存储的内存地址：\na[i]_address = base_address + i * data_type_size\rdata_type_size 表示数组中每个元素的大小。\n数组的插入和删除\r#\r\r为什么说数组的插入和删除效率低？\n比如一个数组的长度为 n，如果要插入一个元素在 k 位置，时间复杂度是多少?\n如果在数组的末尾插入，那么不需要移动数据，在末尾加入元素即可，这时是最好时间复杂度，为 O(1)。如果在数组的开头插入元素，那么所有元素一次向 后移动一位，这时是最坏时间复杂度，为 O(n)。由于在每个位置插入的概率是一样的，那么平均时间复杂度为 (1 + 2 + ...n)/n = O(n)。\n删除元素和插入元素差不多。如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况 时间复杂度也为 O(n)。如果数组是无序的，多次删除操作可以集中在一起执行。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组 没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。\n如何避免移动数据\r#\r\r如果数组是有序的，那么插入元素就必须移动数据。对于无序的数组，如果要在 k 位置插入元素，为了避免移动数据，可以直接将 k 位置的原数据移动到 数据末尾，把新的元素放在 k 位置。\n为什么数组的下标都是从 0 开始\r#\r\r下标就是\u0026quot;偏移 (offset)\u0026quot;，上面的例子中数组 a 的首地址是 a[0]，也就是偏移为 0 的位置。a[k] 就表示偏移 k 个 data_type_size 的 位置，那么 a[k] 内存地址为 a[k]_address = base_address + k * data_type_size。\n但是如果数组从 1 开始，a[k] 内存地址就是 a[k]_address = base_address + (k - 1) * data_type_size，可以看出 多了一次减法运算， CPU 就多一次减法指令。对于数组这种基础数据结构，少一次操作就可以提高一点效率。\n也可能是历史原因，C 语言是从 0 开始，之后的语言也都效仿了 C 语言。\n数组越界\r#\r\rfunc OutOffArray() { i := 0 var a [3]int for ; i \u0026lt;= 3; i ++ { a[i] = 0 fmt.Printf(\u0026#34;hello %d.\\n\u0026#34;, i) } } 数组大小为 3，a[0]，a[1]，a[2]，但是上面的代码循环的结束条件错写为了 i\u0026lt;=3 而非 i\u0026lt;3，所以当 i=3 时，数组 a[3] 访问 越界。\n在 Go 中会报错：\npanic: runtime error: index out of range [recovered] panic: runtime error: index out of range 但是在 C 语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据数组寻址公式，a[3] 也会被定位到某块不属于数组的内存 地址上。数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要 数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。\n为什么 Javascript 的数组可以存储不同类型的数据\r#\r\rJavascript 基本类型数据都是直接按值存储在栈中的(Undefined、Null、布尔、数字和字符串)，每种类型的数据占用的内存空间的大小是确定的，并由系统 自动分配和自动释放。这样带来的好处就是，内存可以及时得到回收，相对于堆来说，更加容易管理内存空间。\nJavascript 引用类型数据被存储于堆中 (如对象、数组、函数等，它们是通过拷贝和 new 出来的），但是引用类型的数据的地址指针是存储于栈中的。 当想要访问引用类型的值的时候，需要先从栈中获得对象的地址指针，然后，在通过地址指针找到堆中的所需要的数据。\nJavascript 中数组类型与其他语言不太一样：\n 数组中可以存放不同的数据结构，可以存放数组、对象、Number、Undefined、Null、String、Symbol、Boolean、Function 等等。 数组的 index 是字符串类型的，之所以你可以通过 arr[1]，获得对应的数据，是因为 Javascript 自动将数字转化为字符串。  Javascript 中数组 JSArray 是继承自 JSObject，也就是说，数组是一个特殊的对象。内部也是 key-value 的存储形式。\nJS 的数组是以类似哈希表的方式存在的。key 为 0，1，2，3 ... 这种索引，value 就是数组的元素对于读取操作，哈希表的效率并不高，而修改删除 的效率比较高。\n现代浏览器对数组的内存分配进行了优化：\n 对于同构的数组，也就是，数组中元素类型一致，会创建连续的内存分配 对于不同构数组，按照哈希表方式创建 如果你想插入一个异构数据，那么就会重新解构，以哈希表方式重新创建 新创建的空数组，默认的创建连续的内存分配  Javascript 中提供了 ArrayBuffer 对象，它可以创建连续的内存。\nhttps://juejin.im/post/5d80919b51882538036fc87d#2\n"});index.add({'id':17,'href':'/algorithm-learn/docs/04_stack/','title':"栈",'content':"栈\r#\r\r关于“栈”的一个例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取， 不能从中间任意抽出。先进后出，这就是典型的“栈”结构。\n栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。\n某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，就应该首选“栈”这种数据结构。\n栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，叫作顺序栈，用链表实现的栈，叫作链式栈。\n对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样了。 当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。\n支持动态扩容的顺序栈\r#\r\r基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈 的大小不受限，但要存储 next 指针，内存消耗相对较多。如何基于数组实现一个可以支持动态扩容的栈？\n如果要实现一个支持动态扩容的栈，就需要底层依赖一个支持动态扩容的数组。\n栈在表达式求值中的应用\r#\r\r编译器如何利用栈来实现表达式求值，比如：34+13*9+44-12/3。\n使用两个栈，其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，就直接压入操作数栈；当遇到运算符， 就与运算符栈的栈顶元素进行比较。\n如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈 的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n如何实现浏览器的前进、后退功能\r#\r\r使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当点击前进按 钮时，依次从栈Y中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面 可以点击前进按钮浏览了。\n"});index.add({'id':18,'href':'/algorithm-learn/docs/13_binary_tree/','title':"树",'content':"树\r#\r\r栈、队列等都是线性表结构，树，一种非线性表结构，这种数据结构比线性表的数据结构要复杂得多。\n树的每个元素叫作节点，用来连线相邻节点之间的关系，叫作父子关系。\nA 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互 称为兄弟节点。没有父节点的节点叫作根节点，也就是节点 E。没有子节点的节点叫作叶子节点或者叶节点， G、H、I、J、K、L 都是叶子节点。\n“树” 的三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。\n二叉树\r#\r\r二叉树是最常用的树结构。\n二叉树，就是每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点， 有的节点只有左子节点，有的节点只有右子节点。\n 编号 2，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。 编号 3 ，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大， 这种二叉树叫作完全二叉树。  二叉树的存储\r#\r\r为什么把最后一层的叶子节点靠左排列的叫完全二叉树？\n二叉树的存储方法：\n 基于指针或者引用的二叉链式存储法。 基于数组的顺序存储法。  链式存储法\r#\r\r上图是链式存储法，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。只要拎住根节点，就可以通过左右子节点 的指针，把整棵树都串起来。这种存储方式比较常用。大部分二叉树代码都是通过这种结构来实现的。\n顺序存储法\r#\r\r上图是顺序存储法，节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储 的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。只要知道根节点存储的位置，就可以把整棵树都串起来。\n但是基于数组的顺序存储法，如果是非完全二叉树，其实会浪费比较多的数组存储空间。比如下面途中的例子：\n如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要存储额外的左右子节点的指针。 这就是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n堆其实就是一种完全二叉树，最常用的存储方式就是数组。\n二叉树的遍历\r#\r\r遍历方法有三种：\n 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。  每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是 O(n)。\n二叉查找树\r#\r\r二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。支持快速查找、插入、删除一个数据。\n二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。\n查找\r#\r\r先取根节点，如果它等于要查找的数据，就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大， 那就在右子树中递归查找。\n插入\r#\r\r新插入的数据一般都是在叶子节点上，所以只需要从根节点开始，依次比较要插入的数据和节点的大小关系。如果要插入的数据比节点的数据大，并且节点的 右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点 的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。\n删除\r#\r\r删除操作比较复杂，针对要删除节点的子节点个数的不同，要分三种情况：\n 如果要删除的节点没有子节点，只需要直接将父节点中，指向要删除节点的指针置为 null。 如果要删除的节点只有一个子节点（只有左子节点或者右子节点），只需要更新父节点指向要删除节点的指针，让它指向要删除节点的子 节点就可以了。 如果要删除的节点有两个子节点，这就比较复杂了。需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个 最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。  支持重复数据的二叉查找树\r#\r\r上面的二叉查找树的操作，针对的是不存在键值相同的情况。那如果存储的两个对象键值相同，该怎么处理？ 有两种解决方法：\n 二叉查找树中每一个节点不仅会存储一个数据，因此通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。 第二种方法，每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的 数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。  当要查找数据的时候，遇到值相同的节点，并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所 有节点都找出来。\n对于删除操作，也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n二叉查找树的时间复杂度\r#\r\r不同形态二叉查找树，查找、插入、删除操作的时间复杂度是不同的。\n第一种情况是最糟糕的情况，根节点的左右子树极度不平衡，退化成了链表，查找的时间复杂度就变成了 O(n)。\n最理想的情况下，二叉查找树是一棵完全二叉树（或满二叉树），时间复杂度其实都跟树的高度成正比，也就是 O(height) （可以查看代码示例）。\n那么如何求一棵包含 n 个节点的完全二叉树的高度？\n树的高度就等于最大层数减一，包含 n 个节点的完全二叉树中，第一层包含 1 个节点，第二层包含 2 个节点，第三层包含 4 个节点，依次类推，下面一 层节点个数是上一层的 2 倍，第 K 层包含的节点个数就是 2^(K-1)。\n但是，对于完全二叉树来说，最后一层的节点个数并不确定。它包含的节点个数在 1 个到 2^(L-1) 个之间（假设 L 是最大层数）。 如果把每一层的节点个数加起来就是总的节点个数 n。也就是说，如果节点的个数是 n，那么 n 满足这样一个关系：\nn \u0026gt;= 1+2+4+8+...+2^(L-2)+1\rn \u0026lt;= 1+2+4+8+...+2^(L-2)+2^(L-1)\r这时两个等比数列，L 的范围是[log2(n+1), log2(n) +1]。完全二叉树的层数小于等于 log2(n) + 1，也就是说，完全二叉树的高度小于 等于 log2n。\n二叉查找树和散列表\r#\r\r散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度 才是 O(logn)，相对散列表，好像并没有什么优势，那为什么还要用二叉查找树？\n 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间 复杂度内，输出有序的数据序列。 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，最常用的平衡二叉查找树的性能非常稳定，时间 复杂度稳定在 O(logn)。 尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性 这一个问题。 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。  红黑树\r#\r\r极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。要解决这个复杂度退化的问题，需要设计一种平衡二叉查找树。\n平衡二叉查找树\r#\r\r平衡二叉树：二叉树中任意一个节点的左右子树的高度相差不能大于 1。从这个定义来看，完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有 可能是平衡二叉树。\n红黑树\r#\r\r平衡二叉查找树有很多，比如，Splay Tree（伸展树）、Treap（树堆）等，红黑树（Red-Black Tree），简称 R-B Tree。它是一种不严格的平 衡二叉查找树。\n红黑树中的节点，一类被标记为黑色，一类被标记为红色。一棵红黑树要满足的要求：\n 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；  平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能 不会退化的太严重。红黑树是近似平衡的。\n"});index.add({'id':19,'href':'/algorithm-learn/docs/24_bmore_tree/','title':"索引",'content':"对于存储的需求，无非就是增删改查，并不复杂。但是，一旦存储的数据很多，那性能就成了要关注的重点。 特别是一些跟存储相关的基础系统，比如 MySQL 数据库、消息中间件 RocketMQ 等。这些系统的实现，都离不开索引。\n索引可以类比书籍的目录来理解，通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。\n设计索引的需求\r#\r\r功能性需求\r#\r\r功能性需求需要考虑的点：\n 数据可以分为两类：   结构化数据，比如，MySQL 中的数据； 非结构化数据，比如搜索引擎中网页。非结构化数据一般需要做预处理，提取出查询关键词，对关键词构建索引。  数据是静态还是动态：   静态数据，也就是说，不会有数据的增加、删除、更新操作，在构建索引的时，只需要考虑查询效率就可以了。 动态数据，大部分场景下，都是对动态数据构建索引，不仅要考虑到索引的查询效率，在原始数据更新的同时， 还需要动态地更新索引。设计起来更加复杂。  索引的存储：   索引存储在内存中，查询的速度肯定要比在磁盘中高。但是，如果原始数据量很大的情况下，对应的索引可能也会 很大。内存是有限的，这时就需要考虑存储在磁盘。 一部分存储在内存，一部分存储在磁盘，可以兼顾内存消耗和查询效率。  单值查找还是区间查找：   单值查找，也就是根据查询关键词等于某个值的数据。 区间查找，就是查找关键词处于某个区间值的所有数据。  单关键词查找还是多关键词组合查找：   搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如 “数据结构 AND 算法”。 对于多关键词查询，像 MySQL 这种结构化数据的查询需求，可以实现针对多个关键词的组合，建立索引； 对于像搜索引擎这样的非结构数据的查询需求，可以针对单个关键词构建索引，然后通过集合操作，比如求并集、 求交集等，计算出多个关键词组合的查询结果。  非功能性需求\r#\r\r存储空间\r#\r\r不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非 常有限，一个中间件启动后就占用几个 GB 的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。 但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。\n索引的维护成本\r#\r\r索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，也需要 动态的更新索引。而索引的更新势必会影响到增删改操作的性能。\n构建索引的数据结构\r#\r\r常用来构建索引的数据结构，比如，散列表、红黑树、跳表、B+ 树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来 对静态数据构建索引。\n散列表增删改查操作的性能非常好，时间复杂度是 O(1)。一些键值数据库，比如 Redis、Memcache，就是使用散列表来构建索引的。\n位图和布隆过滤器这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。布隆过滤器有一定的判错率。但是对于判定 不存在的数据，那肯定就不存在。而且，内存占用非常少。可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，可以先通 过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。\nB+ 树索引\r#\r\r假设有两个需求：\n 根据某个值查找数据，比如 select * from user where id=1234； 根据区间值来查找某些数据，比如 select * from user where id \u0026gt; 1234 and id \u0026lt; 2345。  上面的语句实现了功能需求，除了功能，对于数据库的性能要考察两个方面：\n 执行效率，希望通过索引，查询数据的效率尽可能的高； 存储空间，希望索引不要消耗太多的内存空间。  用数据结构解决问题\r#\r\r 散列表的查询性能很好，时间复杂度是 O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足需求。 平衡二叉查找树查询的性能也很高，时间复杂度是 O(logn)。而且，对树进行中序遍历，可以得到一个从小到大有序的数据序列， 但这仍然不足以支持按照区间快速查找数据。 跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是 O(logn)。并且，跳表也支 持按照区间快速地查找数据。只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对 应的结点为止，这期间遍历得到的数据就是满足区间值的数据。  跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作 B+ 树。不过，它是通过二叉查找树演化过来的， 而非跳表。\n改造二叉查找树来解决问题\r#\r\r为了让二叉查找树支持按照区间来查找数据，可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，把每个叶 子节点串在一条链表上，链表中的数据是从小到大有序的。就像下图，看起来很像跳表：\n如果要求某个区间的数据。只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，再顺着链表往后遍历，直到链表中的结点数据值大于 区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。\n但是，如果要为几千万、上亿的数据构建索引，存储在内存中，占用的内存会非常多。比如，给一亿个数据构建二叉查找树索引，那索引中会包含 大约 1 亿个节点，每个节点假设占用 16 个字节，那就需要大约 1GB 的内存空间。\n如何占用太多内存的问题\r#\r\r借助时间换空间的思路，把索引存储在硬盘中，而非内存中。但是硬盘是一个非常慢速的存储设备。如果把改造之后的二叉树存储在硬盘中，那么每 个节点的读取（或者访问），都对应一次磁盘 IO 操作。树的高度就等于每次查询数据时磁盘 IO 操作的次数。\n如何降低树的高度\r#\r\r把索引构建成 m 叉树.\n如下图，给 16 个数据构建二叉树索引，树的高度是 4，查找一个数据，就需要 4 个磁盘 IO 操作（如果根节点存储在内存中，其他结点存储在磁盘中）， 如果对 16 个数据构建五叉树索引，那高度只有 2，查找一个数据，对应只需要2次磁盘操作。如果 m 叉树中的 m 是 100，那对一亿个数据构建索引，树 的高度也只是 3，最多只要 3 次磁盘 IO 就能获取到数据。磁盘 IO 变少了，查找数据的效率也就提高了。\nm 叉树 m 的大小\r#\r\r操作系统都是按页（一页大小通常是 4KB，这个值可以通过 getconfig PAGE_SIZE 命令查看）来读取的，一次会读一页的数据。如果要读取的数 据量超过一页的大小，就会触发多次 IO 操作。所以，在选择 m 大小的时候，要尽量让每个节点的大小等于一个页的大小。\n索引的弊端\r#\r\r索引也会让写入数据的效率下降。数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。\n对于一个 B+ 树来说，m 值是根据页的大小事先计算好的，也就是说，每个节点最多只能有 m 个子节点。在往数据库中写入数据的过程中，这样就 有可能使索引中某些节点的子节点个数超过 m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘 IO 操作。\n需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过m个。不过这也没关系，我们可以用同样的方法，将 父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。\n正是因为要时刻保证 B+ 树索引是一个 m 叉树，所以，索引的存在会导致数据库写入的速度降低。\n"});index.add({'id':20,'href':'/algorithm-learn/docs/08_linear_sort/','title':"线性排序",'content':"线性排序\r#\r\r如何根据年龄给 100 万用户排序？如果使用归并、快排，可以完成功能，但是时间复杂度最低也是 O(nlogn)。有没有更快的排序方法？\n桶排序\r#\r\r桶排序，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数 据按照顺序依次取出，组成的序列就是有序的了。\n桶排序的时间复杂度\r#\r\r如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度 为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。 当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。\n桶排序对数据的要求\r#\r\r桶排序对要排序数据的要求非常苛刻：\n 要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要 再进行排序。 数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度 就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。  比如说有 10GB 的订单数据，如果希望按订单金额（假设金额都是正整数）进行排序，但是内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载 到内存中。怎么做？\n借助桶排序的处理思想： 先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个 桶里，第一个桶存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且 按照金额范围的大小顺序编号命名（00，01，02…99）。\n理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将 这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写 入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。\n但是，订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的 数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。怎么处理？\n针对这些划分之后还是比较大的文件，可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间， 1 元到 100 元，101 元到 200 元 … 901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续 再划分，直到所有的文件都能读入内存为止。\n计数排序\r#\r\r计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，就可以把数据划分成 k 个桶。每个桶内的数 据值都是相同的，省掉了桶内排序的时间。\n比如高考查分数系统，查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次？\n考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万 考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中， 就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。\n计数排序的算法思想跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫计数排序？\n假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8] 中，它们分别是：2，5，3，0，2，3，0，3。\n成绩从 0 到 5 分，使用大小为 6 的数组 C[6] 表示桶，其中下标对应分数。C[6] 内存储的是对应的考生个数。\nR[8] 为排序之后的有序数组。如何快速计算出，每个分数的考生在有序数组中对应的存储位置？\n思路：对 C[6] 数组顺序求和，C[6] 存储的数据就变成了下面这样子。C[k] 里存储小于等于分数 k 的考生个数。\n从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等 于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元 素就只剩下了 6 个了，所以相应的 C[3] 要减 1，变成 6。\n以此类推，当扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 R 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数 组 A 后，数组 R 内的数据就是按照分数从小到大有序排列的了。\n计数排序就是利用另外一个数组来计数。计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。 计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。\n基数排序\r#\r\r假设有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，有什么比较快速的排序方法？\n手机号码有 11 位，范围太大，显然不适合用桶排序和计数排序。有没有比快排更高效的排序算法？基数排序。\n手机号码有这样的规律：假设要比较两个手机号码 a，b 的大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了。\n先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。\n基数排序的过程分解图： 根据每一位来排序，可以用桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数 排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 O(n)。\n排序优化\r#\r\r选择合适的排序算法\r#\r\r 线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。 小规模数据进行排序，可以选择时间复杂度是 O(n^2)的算法 大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效 为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数  优化快速排序\r#\r\r快速排序在最坏情况下的时间复杂度是 O(n^2)，如何来解决这个“复杂度恶化”的问题？\n为什么最坏情况下快速排序的时间复杂度是 O(n^2)？如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序 算法就会变得非常糟糕，时间复杂度就会退化为 O(n^2)。实际上，这种 O(n^2) 时间复杂度出现的主要原因还是因为分区点选的不够合理。\n最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。\n三数取中法\r#\r\r从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。如果要排序的数组比较大，那“三数取中”可能就不够了， 可能要“五数取中”或者“十数取中”。\n随机法\r#\r\r随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能 会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。\n分析排序函数\r#\r\r以 Glibc 中的 qsort() 函数举例，从名字上看，好像是基于快排实现的，实际上它并不仅仅用了快排这一种算法。\nqsort() 会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是 O(n)，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外 需要 1KB、2KB 的内存空间，这个问题不大。这就是空间换时间。\n如果数据量太大，比如 100MB 的数据，这个时候 qsort() 会改为用快速排序算法来排序。qsort() 选择分区点的方法就是“三数取中法”。\nqsort() 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时， qsort() 就退化为插入排序，不再继续用递归来做快速排序，\n"});index.add({'id':21,'href':'/algorithm-learn/docs/19_greedy/','title':"贪心算法",'content':"贪心、分治、回溯、动态规划是 4 种基本的算法思想。\n理解贪心算法\r#\r\r假设有一个可以容纳 100kg 物品的背包，有 5 种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大， 如何选择在背包中装哪些豆子？每种豆子又该装多少？\n   种类 总量（kg） 总价值（元）     黄豆 100 100   绿豆 30 90   红豆 60 120   黑豆 20 80   青豆 50 70    这个问题很简单，只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。这个问题的解决思路借助的就是贪心算法。\n碰到这类问题，首先要联想到贪心算法：针对一组数据，定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下， 期望值最大。上面的例子，限制值就是重量不能超过 100kg，期望值就是物品的总价值。\n贪心算法解决问题的思路，并不总能给出最优解，例如： 在一个有权图中，从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟 当前顶点相连的权最小的边，直到找到顶点 T。按照这种思路，求出的最短路径是 S-\u0026gt;A-\u0026gt;E-\u0026gt;T，路径长度是 1+4+4=9。\n但是实际上最短的路径是 S-\u0026gt;B-\u0026gt;D-\u0026gt;T，长度是 2+2+2=6。\n在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果第一步从顶点 S 走到顶点 A，那接下来面对的顶 点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便第一步选择最优的走法（边最短），但有可能因为这一步选择，导 致后面每一步的选择都很糟糕。\n实战分析\r#\r\r分糖果\r#\r\r有 m 个糖果和 n 个孩子。但是糖果少，孩子多（m\u0026lt;n），所以糖果只能分配给一部分孩子。\n每个糖果的大小不等，这 m 个糖果的大小分别是 s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有 糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这 n 个孩子对糖果大小的需求分 别是 g1，g2，g3，……，gn。如何分配糖果，能尽可能满足最多数量的孩子？\n如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求 更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子 跟满足一个需求小的孩子，对期望值的贡献是一样的。\n钱币找零\r#\r\r假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。 现在要用这些钱来支付 K 元，最少要用多少张纸币？\n在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。\n区间覆盖\r#\r\r假设有 n 个区间，区间的起始端点和结束端点分别是 [l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。从这 n 个区间中选出一部分 区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间？\n解决思路：假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于，选择几个不相交的区间， 从左到右将 [lmin,rmax] 覆盖上。按照起始端点从小到大的顺序对这 n 个区间排序。\n每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置 更多的区间。这实际上就是一种贪心的选择方法。\n霍夫曼编码\r#\r\r假设我有一个包含 1000 个字符的文件，每个字符占 1 个 byte，那么一共需要 8000bits，有没有更加节省空间的存储方式？\n假设通过统计分析发现，这 1000 个字符中只包含 6 种不同字符，假设它们分别是 a、b、c、d、e、f。而 3 个二进制位（bit）就可 以表示 8 个不同的字符，所以，为了尽量减少存储空间，每个字符用 3 个二进制位来表示。那存储这 1000 个字符只需要 3000bits 就可 以了，比原来的存储方式节省了很多空间。不过，还有没有更加节省空间的存储方式？\n霍夫曼编码就要登场了。霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。\n霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码 试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码？根据贪心的思想，把出现频率比 较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。\n对于等长的编码来说，解压缩起来很简单。比如刚才那个例子中，用 3 个 bit 表示一个字符。在解压缩的时候，每次从文本中读取 3 位二 进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取 1 位还是 2 位、3 位等等来解压缩？ 为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。\n假设这 6 个字符出现的频率从高到低依次是 a、b、c、d、e、f。把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀， 在解压缩的时候，每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这 1000 个字符只需 要 2100bits 就可以了。\n如何根据字符出现频率的不同，给不同的字符进行不同长度的编码？\n把每个字符看作一个节点，并且辅带着把频率放到优先级队列中。从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C，把频率设 置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点。最后再把 C 节点放入到优先级队列中。重复这个过程，直到队 列中没有数据。\n给每一条边加上画一个权值，指向左子节点的边我们统统标记为 0，指向右子节点的边，统统标记为 1，那从根节点到叶节点的路径就 是叶节点对应字符的霍夫曼编码。\n"});index.add({'id':22,'href':'/algorithm-learn/docs/10_skip_list/','title':"跳表",'content':"跳表\r#\r\r二分查找底层依赖的是数组随机访问的特性，如何用链表实现类似二分查找的算法，就需要对链表稍加改造，这种改造之后的数据结构叫作 跳表（Skip list）。\n对于一个单链表，即使链表中存储的数据是有序的，如果要想在其中查找某个数据，也只能从头到尾遍历链表。时间复杂度会是 O(n)。\n如果对链表建立一级“索引”，每两个结点提取一个结点到上一级，抽出来的那一级叫作索引或索引层。图中的 down 表示 down 指针，指向下一级结点。\n如果现在要查找值为 16 节点。可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，发现下一个结点的值是 17，那要查找 的结点 16 肯定就在这两个结点之间。然后通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，只需要 再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。\n加来一层索引之后，查找效率提高了。那如果再加一级索引效率会不会提升更多？\n上图中，再来查找 16，只需要遍历 6 个结点，需要遍历的结点数量又减少了。\n上图中，一个包含 64 个结点的链表，按照前面思路，建立了五级索引。原来没有索引的时候，查找 62 需要遍历 62 个结点，现在 只需要遍历 11 个结点，速度提高了很多。所以，当链表的长度 n 比较大时，比如 1000、10000 的时候，在构建索引之后，查找效 率的提升就会非常明显。\n链表加多级索引的结构，就是跳表。\n跳表的时间复杂度\r#\r\r如果链表里有 n 个结点，按照前面的思路，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二 级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第k级索引的结点个数是第 k-1 级索引的结 点个数的 1/2，那第 k 级索引结点的个数就是 n/(2^k)。\n假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2^h)=2，从而求得 h=log2(n-1)。如果包含原始链 表这一层，整个跳表的高度就是 log2(n)。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数 据的时间复杂度就是 O(m*logn)。\n这个 m 的值是多少？按照前面这种索引结构，每一级索引都最多只需要遍历 3 个结点，也就是说 m=3，为什么是 3？\n假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索 引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。\n所以在跳表中查询任意数据的时间复杂度就是 O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说， 其实是基于单链表实现了二分查找。这种查询效率的提升，前提是建立了很多级索引，也就需要更多的内存，也就是空间换时间的设计思路。\n跳表的空间复杂度\r#\r\r跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间？\n假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。把每层索引的结点数写出来，就是一个等比数列。\n这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是 O(n)。也就是说，如果将包含 n 个结点的 单链表构造成跳表，需要额外再用接近 n 个结点的存储空间。有没有办法降低索引占用的内存空间？\n可以每三个结点或五个结点，抽一个结点到上级索引，比如个每三个结点抽一个，第一级索引需要大约 n/3 个结点，第二级索引需要 大约 n/9 个结点。每往上一级，索引结点个数都除以 3。为了方便计算，假设最高一级的索引结点个数是 1。每级索引的结点个数都写下来， 也是一个等比数列。\n通过等比数列求和公式，总的索引结点大约就是 n/3+n/9+n/27+…+9+3+1=n/2。尽管空间复杂度还是 O(n)，但比上面的每两个结点 抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。\n高效的动态插入和删除\r#\r\r跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。\n插入\r#\r\r在单链表中，插入结点的时间复杂度是 O(1)。耗时的是查找，跳表查找操作时间复杂度是 O(logn)。所有插入的时间复杂度 也是 O(logn)。\n删除\r#\r\r如果这个结点在索引中也有出现，除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结 点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果用的是双向链表，就 不需要考虑这个问题了。\n跳表索引动态更新\r#\r\r不停地往跳表中插入数据时，如果不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。\n作为一种动态数据结构，需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增 加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n跳表是通过随机函数来维护“平衡性”。当往跳表中插入数据的时候，可以选择同时将这个数据插入到部分索引层中。通过一个随机函数 来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那就将这个结点添加到第一级到第 K 级这 K 级索引中。\n"});index.add({'id':23,'href':'/algorithm-learn/docs/06_recursion/','title':"递归",'content':"递归\r#\r\r比如在影院，你不知道你现在坐在第几排，就可以问前一排的人他在第几排，如果前面的人也不知道，再问前一排人，直到问道第一排。再把这个数字 一排一排传回来，就知道在第几排了。这个过程就是递归。去的过程是“递”，回来的过程是“归”。\n递归需要的三个条件\r#\r\r 一个问题的解可以分解为几个子问题的解何为子问题？子问题就是数据规模更小的问题。 比如你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样。 比如你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。 存在递归终止条件。 例子中第一排的人不需要再继续询问任何人，就知道自己在哪一排，这就是递归的终止条件。  递归代码最关键的是写出递推公式，找到终止条件。\n堆栈溢出\r#\r\r函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟 机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。\n比如前面电影院的例子，如果我们将系统栈或者 JVM 堆栈大小设置为 1KB，在求解 f(19999) 时便会出现如下堆栈报错： Exception in thread \u0026quot;main\u0026quot; java.lang.StackOverflowError\n如何避免出现堆栈溢出\r#\r\r可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了， 直接返回报错。\n重复计算\r#\r\r使用递归时还会出现重复计算的问题。可以通过一个数据结构（比如散列表）来保存已经求解过的递归函数。\n"});index.add({'id':24,'href':'/algorithm-learn/docs/03_link_list/','title':"链表",'content':"链表\r#\r\r链表也是一种基础的数据结构。常见的链表结构有：单链表，双向链表，循环链表。\n单链表\r#\r\r链表通过指针将一组零散的内存块串联在一起。其中内存块称为链表的结点。为了将所有的结点串起来，每个链表的结点除了存储数据之外， 还需要记录链上的下一个结点的地址。这个记录下个结点地址的指针叫作后继指针 next。\n其中有两个结点是比较特殊的，它们分别是头结点和尾结点。其中，头结点用来记录链表的基地址。有了它，就可以遍历得到整条链表。而尾结点 的指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。\n在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一 个数据是非常快速的。链表的插入和删除操作，只需要考虑相邻结点的指针改变，时间复杂度是 O(1)。\n虽然插入和删除变得高效了，但是这也导致链表的随机访问没有数组效率高，因为无法像数组那样直接通过寻址公式计算出下标对应的内存地址，必须 根据后继指针来遍历每一个节点。所以链表的随机减访问时间复杂度为 O(n)。\n循环链表\r#\r\r循环链表只是一种特殊的单链表，唯一的不同就是循环链表的尾节点的指针不指向空地址 NULL，而是头节点。\n循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如约瑟夫问题。\n双向链表\r#\r\r单向链表只有一个方向，而双向链表，有两个方向。每个结点不止有一个后继指针 next 指向后面的结点，还有一个 前驱指针 prev 指向前面的结点。\n双向链表比单链表需要额外的空间来存储前驱指针，因此同样的数据要比单链表占用更多的内存空间。\n双向链表的优势\r#\r\r双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。\n  删除结点中“值等于某个给定值”的结点 对于这种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点， 然后再通过我前面讲的指针操作将其删除。尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。 根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。\n  删除给定指针指向的结点 这种情况，加入我们已经找到了要删除的结点 q，得到了 q 节点的指针，但是要删除删除这个结点 q 还需要知道其前驱结点，而单链表并不支持直接获取 前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-\u0026gt;next=q，说明 p 是 q 的前驱结点。但是对于双向链表来说，因为 双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对这种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表 只需要 O(1) 的时间复杂度。\n  对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系， 决定是往前还是往后查找，所以平均只需要查找一半的数据。\n与数组的区别\r#\r\r 数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时， 即便内存的剩余总可用空间大于 100MB，仍然会申请失败。 链并不需要一块连续的内存空间，它通过指针将一组零散的内存块串联起来使用。  性能\r#\r\r 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续 存储，所以对 CPU 缓存不友好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致 “内存不足（out ofmemory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去， 非常费时。链表本身没有大小的限制，天然地支持动态扩容，动态扩容是链表与数组最大的区别。  利用哨兵简化实现难度\r#\r\r单链表的插入和删除操作。如果在结点 p 后面插入一个新的结点，只需要两行代码就可以搞定。\nnew_node-\u0026gt;next = p-\u0026gt;next; p-\u0026gt;next = new_node; 但是，当要向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。需要进行特殊处理，其中 head 表示链表的头结点。\nif (head == null) { head = new_node; } 如果要删除结点 p 的后继结点：\np-\u0026gt;next = p-\u0026gt;next-\u0026gt;next; 但是，如果要删除链表中的最后一个结点，前面的删除代码就不 work 了。需要特殊处理：\nif (head-\u0026gt;next == null) { head = null; } 针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也 容易因为考虑不全而出错。\n哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。还记得如何表示一个空链表吗？ head=null 表示链表中没有结点了。其中 head 表示头结点指针，指向链表中的第一个结点。如果引入哨兵结点，在任何时候，不管链表是不是空， head 指针都会一直指向这个哨兵结点。这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。 哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码 实现逻辑了\nLRU\r#\r\r常见的缓存淘汰策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用 策略 LRU（Least Recently Used）。\n如何基于链表实现 LRU 缓存淘汰算法\r#\r\r思路：维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，从链表头开始顺序遍历链表。\n 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况：   如果此时缓存未满，则将此结点直接插入到链表的头部； 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。  实现 LRU 算法，用数组也可以的，但是要考虑性能，选择最优的数据结构。\n"});index.add({'id':25,'href':'/algorithm-learn/docs/05_queue/','title':"队列",'content':"队列\r#\r\r队列，先进先出，像排队买票一样。队列和栈很像，也是一种操作受限的线性表数据结构，有两个基本操作：\n 入队（enqueue），放一个数据到队列尾部。 出队（dequeue），从队列头部取一个元素。  顺序队列和链式队列\r#\r\r顺序队列\r#\r\r跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。\ntype ArrayQueue struct { items []string n int // 数组 size \thead int // 队列头下标 \ttail int // 队列尾下标 } func (a *ArrayQueue) NewArrayQueue(capacity int) { a.items = []string{} a.n = capacity a.head = 0 a.tail = 0 } // 入队 func (a *ArrayQueue) Enqueue(item string) bool { if a.tail == a.n { // 队列已满 \treturn false } a.items[a.tail] = item a.tail ++ return true } // 出队 func (a *ArrayQueue) Dequeue() string { if a.head == a.tail { // 队列为空 \treturn \u0026#34;\u0026#34; } item := a.items[a.head] a.head ++ return item } 上面的实现比栈的数组实现稍微复杂，栈只需要一个栈顶指针，但是队列需要一个 head 指针和一个 tail 指针，分别指向队列的头和尾。\n随着不停地进行入队、出队操作，head 和 tail 都会持续往后移动。当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中 添加数据了。这个问题如何解决？\n在栈的数组实现中，通过数据搬移，但是，对于队列来说每次进行出队操作都相当于删除数组下标为 0 的数据，要搬移整个队列中的数据，这样出 队操作的时间复杂度就会从原来的 O(1) 变为 O(n)。\n在出队时可以不用搬移数据。如果没有空闲空间了，只需要在入队时，再集中触发一次数据的搬移操作。修改 Enqueue 函数：\nfunc (a *ArrayQueue) Enqueue(item string) bool { if a.tail == a.n { // 队列已满 \tif a.head == 0 { // 没有空闲空间 \treturn false } // 队列已满 但是有空闲空间 进行数据搬移 \tfor i := a.head; i \u0026lt; a.tail; i ++ { // 从 head 指向的头部数据开始搬移 \ta.items[i - a.head] = a.items[i] } // 数据搬移后 更新 head tail \ta.tail -= a.head a.head = 0 } a.items[a.tail] = item a.tail ++ return true } 链式队列\r#\r\r基于链表实现，同样需要两个指针：head 指针和 tail 指针。它们分别指向链表的第一个结点和最后一个结点。如图所示， 入队时，tail-\u0026gt;next= new_node, tail =tail-\u0026gt;next；出队时，head = head-\u0026gt;next。\ntype LinkedListQueue struct { head *ListNode tail *ListNode Length int } type ListNode struct { data interface{} next *ListNode } func NewLinkedListQueue() *LinkedListQueue { return \u0026amp;LinkedListQueue{nil, nil, 0} } // 入队 func (l *LinkedListQueue) Enqueue(item interface{}) { node := \u0026amp;ListNode{item, nil} if nil == l.tail { // 空队列 \tl.tail = node l.head = node } else { l.tail.next = node l.tail = node } l.Length ++ } // 出队 func (l *LinkedListQueue) Dequeue() interface{} { if l.head == nil { // 队列为空 \tl.tail = nil // clean \treturn nil } item := l.head.data l.head = l.head.next l.Length -- return item } 循环队列\r#\r\r用数组实现的队列，会有数据搬移，影响性能。如何避免？使用循环队列。\n循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。把首尾相连，变成了一个环。\n图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。 这样就避免了数据搬移操作。\n在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。那针对循环队列，如何判断队空和队满？ 队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。\n图中队满的情况，tail=3，head=4，n=8，总结一下规律就是：(3+1)%8=4。尝试多种队满的情况，得出结论，当队满时，(tail+1)%n=head。\n当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。\n"});})();