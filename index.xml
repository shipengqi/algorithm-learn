<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm learning on Algorithm Learning</title>
    <link>http://shipengqi.github.io/algorithm-learn/</link>
    <description>Recent content in Algorithm learning on Algorithm Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    
	<atom:link href="http://shipengqi.github.io/algorithm-learn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AC 自动机</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/18_ac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/18_ac/</guid>
      <description>BF 算法、RK 算法、BM 算法、KMP 算法，都是单模式串匹配算法。Trie 树是多模式串匹配算法。
 单模式串匹配算法，就是在一个主串中查找一个模式串。 多模式串匹配算法，就是在一个主串中查找多个模式串。  AC 自动机（Aho-Corasick）算法，是基于 Trie 树的一种改进算法，它跟 Trie 树的关系，就像单模式串中，KMP 算法与 BF 算法的关系一样。
如何实现敏感词过滤功能#可以针对每个敏感词，通过单模式串匹配算法（比如 KMP 算法）与用户输入的文字内容进行匹配。但是，这样每个匹配过程都需要扫描一遍用户输入的内容。 整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，假如有上千个字符，那就需要扫描几千遍这样的 输入内容。很显然，这种处理思路比较低效。
多模式匹配算法要更高效。如何用 Trie 树实现敏感词过滤功能？
把敏感词字典构建成 Trie 树结构。用户输入的内容作为主串，从第一个字符（假设是字符 C）开始，在 Trie 树中匹配。当匹配到 Trie 树的叶子节点， 或者中途遇到不匹配字符的时候，将主串的开始匹配位置后移一位，也就是从字符 C 的下一个字符开始，重新在 Trie 树中匹配。 这种处理方法，有点类似单模式串匹配的 BF 算法。可以借鉴 KMP 算法对多模式串 Trie 树进行改进。这就需要 AC 自动机算法。
AC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上。</description>
    </item>
    
    <item>
      <title>Trie 树</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/17_trie/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/17_trie/</guid>
      <description>Trie 树#Trie 树（字典树）。树形结构。是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。
例如，一个字符串集合中 6 个字符串： how，hi，her，hello，so，see。如果要在里面多次查找某个字符串是否存在。 如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，效率肯定是比较低的。如何高效查询？
利用 Trie 树结构，利用字符串之间的公共前缀，将重复的前缀合并在一起。如：
根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串 （红色节点并不都是叶子节点）。
构造 Trie 树结构的过程：
当在 Trie 树中查找一个字符串的时候，比如 “her”，将要查找的字符串分割成单个的字符 h，e，r，然后从 Trie 树的根节点开始 匹配。依次找到 h，e，r 三个节点。
如果要查找 “he”，从根节点开始，找到 h，e 节点。但是，路径的最后一个节点 “e” 并不是红色的。也就是说，“he” 是某个字 符串的前缀子串，但并不能完全匹配任何字符串。
实现一棵 Trie 树#二叉树中，一个节点的左右子节点是通过两个指针来存储的，但是 Trie 树是一个多叉树，如何存储子节点？使用数组。 借助散列表的思想，通过一个下标与字符一一映射的数组，来存储子节点的指针。
假设字符串中只有从 a 到 z 这 26 个小写字母，在数组中下标为 0 的位置，存储指向子节点 a 的指针，下标为 1 的位置存储指向子节点 b 的指针， 以此类推，下标为 25 的位置，存储的是指向的子节点 z 的指针。如果某个字符的子节点不存在，就在对应的下标的位置存储 null。
在 Trie 树中查找字符串的时候，就可以通过字符的 ASCII 码减去 a 的 ASCII 码，迅速找到匹配的子节点的指针。比如，d 的 ASCII 码减去 a 的 ASCII 码就是 3，那子节点 d 的指针就存储在数组中下标为 3 的位置中。</description>
    </item>
    
    <item>
      <title>二分查找</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/09_binary_search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/09_binary_search/</guid>
      <description>二分查找#二分查找是一种非常简单易懂的快速查找算法，比如说，一个猜数字游戏。猜一个 0 到 99 之间的数字，猜的过程中，每猜一次，就会告诉你猜的大了还是 小了，直到猜中为止。如何快速猜中数字？
这个例子用的就是二分思想，按照这个思想 7 次就猜出来了。
二分查找的时间复杂度#假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。
其中 n/2^k=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度 就是 O(k)。通过 n/2^k=1，我们可以求得 k=log2(n)，所以时间复杂度就是 O(logn)。
O(logn) 这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1)的算法还要高效。
因为 logn 是一个非常“恐怖”的数量级，即便 n 非常非常大，对应的 logn 也很小。比如 n 等于 2 的 32 次方，这个数很大了吧？大约是 42 亿。 也就是说，如果我们在 42 亿个数据中用二分查找一个数据，最多需要比较 32 次。
用大 O 标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1) 有可能表示的是一个非常大的常量值，比 如 O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高。
实现#func BinarySearch(a []int, value int) int { length := len(a) if length &amp;lt; 1 { return -1 } start := 0 end := length -1 for start &amp;lt;= end { mid := (start + end) / 2 if a[mid] == value { return mid } else if a[mid] &amp;lt; value { start = mid + 1 } else { end = mid - 1 } } return -1 } // 使用递归实现二分查找 func BinarySearchRecursive(a []int, value int) int { length := len(a) if length &amp;lt; 1 { return -1 } return bs(a, value, 0, length -1) } func bs(a []int, value, start, end int) int { if start &amp;gt; end { return -1 } mid := (start + end) / 2 if a[mid] == value { return mid } else if a[mid] &amp;lt; value { return bs(a, value, mid + 1, end) } else { return bs(a, value, start, mid - 1) } } 上面的代码中，mid := (start + end) / 2 这种写法是有问题的。因为如果 start 和 end 比较大的话，两者之和就有可能会溢出。改进的 方法是将 mid 的计算方式写成 start+(end-start)/2。更进一步，如果要将性能优化到极致的话，可以将这里的除以 2 操作转化成位 运算start+((end-start)&amp;gt;&amp;gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。</description>
    </item>
    
    <item>
      <title>位图</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/23_bit_map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/23_bit_map/</guid>
      <description>爬虫 URL 去重#爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。搜索引擎的爬虫系统会爬取几十亿、上百亿的 网页。如何实现 url 去重，避免重复爬取网页。
思路很简单：记录已经爬取的 url。
解析#要记录已经爬取的 url，那么使用什么数据结构来存储。这里涉及到两个操作，添加和查询 url。
散列表，红黑树，跳表都可以实现快速的插入和查找。
散列表#  内存空间占用大 使用散列表存储，如果爬取 10 亿个 url，假设一个 url 长度为 64 bytes，那么存储 10 亿个 url 至少需要大约 60 GB 的内存。 而且散列表要维持较小的装载因子，以免过多出现散列冲突，导致性能下降。而且如果用链表法解决冲突，还要存储链表指针。 所以散列表存储 10 亿 url 需要内存远大于 60 GB，甚至超过 100 GB。
  查询耗时 如果基于链表法解决冲突，散列表中存储的是 URL，那当查询的时候，通过哈希函数定位到某个链表之后，还需要依次比对每个链表中 的 URL。链表中的结点在内存中不是连续存储的，所以不能一下子加载到 CPU 缓存中，没法很好地利用到 CPU 高速缓存，所以数据访 问性能方面会打折扣。
  字符串匹配耗时 链表中的每个数据都是 URL，而 URL 平均长度为 64 字节的字符串，要让待判重的 URL，跟链表中的每个 URL，做字符串匹配。比较耗时。</description>
    </item>
    
    <item>
      <title>分治算法</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/20_deivide_conquer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/20_deivide_conquer/</guid>
      <description>理解分治算法#分治算法（divide and conquer）的核心思想其实就是四个字，分而治之，也就是将原问题划分成 n 个规模较小，并且结构与原问题 相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。
分治有点类似递归的定义。区别是分治算法是一种处理问题的思想，递归是一种编程技巧。 分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：
 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题  分治算法能解决的问题，一般需要满足下面这几个条件：
 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果。  </description>
    </item>
    
    <item>
      <title>动态规划</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/22_dynamic_programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/22_dynamic_programming/</guid>
      <description>动态规划#</description>
    </item>
    
    <item>
      <title>向量空间</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/25_vector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/25_vector/</guid>
      <description>如何实现一个简单的音乐推荐系统？
核心思想：
 找到口味偏好相似的用户，推荐他们爱听的歌曲； 找出跟你喜爱的歌曲特征相似的歌曲，推荐这些歌曲  基于相似用户做推荐#如下图，把听类似歌曲的人，看做口味相似的用户。用 “1” 表示“喜爱”，用 “0” 表示“不发表意见”。从图中可以看出，你跟小明共同喜爱的歌曲最多， 有 5 首。可以得出，小明跟你的口味非常相似。
可以通过用户的行为，来定义对歌曲的喜爱程度。给每个行为定义一个得分，得分越高表示喜爱程度越高。
这里统计两个用户之间的相似度，就需要使用另外一个距离，那就是欧几里得距离（Euclidean distance）。欧几里得距离是用来计算两个向量 之间的距离的。
一维空间是一条线，用 1，2，3 … 这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，用（1，3）（4，2）（2，2）… 这样的两个数， 来表示二维空间中的某个位置；三维空间是一个立体空间，用（1，3，5）（3，1，7）（2，4，3）… 这样的三个数，来表示三维空间中的某个位置。 K 维空间中的某个位置，可以写作 （$X_{1}$，$X_{2}$，$X_{3}$，…，$X_{K}$）。这种表示方法就是向量（vector）。
如何计算两个向量之间的距离#类比到二维、三维空间中距离的计算方法，得到两个向量之间距离的计算公式：
把每个用户对所有歌曲的喜爱程度，都用一个向量表示。计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量：
结论是，小明跟你的口味最相似。
基于相似歌曲做推荐#如何判断两首歌曲是否相似？
对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，给每个歌曲的 每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。</description>
    </item>
    
    <item>
      <title>哈希算法</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/12_hash_algo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/12_hash_algo/</guid>
      <description>哈希算法#不管是“散列”还是“哈希”，英文都是 “Hash”。
哈希算法的定义和原理，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数 据映射之后得到的二进制值串就是哈希值。
实现哈希算法的要求：
 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）； 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同； 散列冲突的概率要很小； 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。  应用#安全加密#哈希算法最先想到的应该就是安全加密。最常用于加密的哈希算法是 MD5（MD5 Message-Digest Algorithm，MD5消息摘要算法）和 SHA（Secure Hash Algorithm，安全散列算法）。
不管是什么哈希算法，只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说？
鸽巢原理（也叫抽屉原理）：它是说，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个。
哈希算法产生的哈希值的长度是固定且有限的。比如 MD5 算法的哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据， 而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。一般情况下， 哈希值越长的哈希算法，散列冲突的概率越低。
唯一标识#如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名 称不同图片内容相同的情况。该如何搜索？
比较笨的办法就是，拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是图片越大，就越耗时。
更好的方法是给每一个图片取一个唯一标识，或者说信息摘要。比如，可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再 取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判 定图片是否在图库中，这样就可以减少很多工作量。
数据校验#BT 下载的原理是基于 P2P 协议的。我们从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每 块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。</description>
    </item>
    
    <item>
      <title>回溯算法</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/21_backtracking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/21_backtracking/</guid>
      <description>回溯算法#</description>
    </item>
    
    <item>
      <title>图</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/15_graph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/15_graph/</guid>
      <description>图#图也是一种非线性表数据结构，比树更复杂。涉及图的算法有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二 分图等等。
树中的元素称为节点，图中的元素叫作顶点（vertex）。
图中的一个顶点可以与任意其他顶点建立连接关系。这种建立的关系叫作边（edge）。
生活中就有很多符合图这种结构的例子。比如，社交网络，就是一个非常典型的图结构。拿微信举例子。可以把每个用户看作一个顶点。如果两个用户之间互加 好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。每个用户有多少个好友，对应到图中，就叫作顶点的度（degree）， 就是跟顶点相连接的边的条数。
微博的社交关系比微信更复杂一点。微博允许单向关注，也就是说，用户 A 关注了用户 B，但用户 B 可以不关注用户 A。就要引入边的“方向”的概念。
这种边有方向的图叫作有向图。边没有方向的图就叫作无向图。
无向图中有“度”这个概念，表示一个顶点有多少条边。在有向图中，把度分为入度（In-degree）和出度（Out-degree）。
顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。
QQ 中的社交关系要还更复杂。QQ 不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经 常往来，亲密度就比较低。
这里就要用到另一种图，带权图（weighted graph）。在带权图中，每条边都有一个权重（weight），可以通过这个权重来表示 QQ 好友 间的亲密度。
图的存储#邻接矩阵#邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，就将 A[i][j] 和 A[j][i] 标记为 1； 对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那就将 A[i][j] 标记为 1。同理，如果有一条箭 头从顶点 j 指向顶点 i 的边，就将 A[j][i] 标记为 1。对于带权图，数组中就存储相应的权重。
邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。
如果存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好几亿的 用户，对应到图上就是好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。
邻接表#邻接表有点像散列表，每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。图中画的是一个有向图的邻接表存储方式，每个顶点对应的 链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点。</description>
    </item>
    
    <item>
      <title>堆</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/14_heap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/14_heap/</guid>
      <description>堆#堆是一种特殊的树。
 堆必须是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右 子节点的值。  对于每个节点的值都大于等于子树中每个节点值的堆，叫作大顶堆。对于每个节点的值都小于等于子树中每个节点值的堆，叫作小顶堆。
将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。
堆最经典的应用就是堆排序了。
实现一个堆#完全二叉树比较适合用数组来存储，非常节省存储空间。
用数组存储堆的例子：
数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储 的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。
插入#往堆中插入一个元素后，需要继续满足堆的两个特性。如果把新插入的元素放到堆的最后，如下图，已经不符合堆的特性。于是， 就需要进行调整，让其重新满足堆的特性，这个过程叫作堆化（heapify）。
堆化有两种，从下往上和从上往下。先看从下往上的堆化方法。
堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。
让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那 种大小关系。
type Heap struct { a []int // 数组，从下标 1 开始存储 	n int // 堆可以存储的元素的最大个数 	count int // 堆已经存储的元素的个数 } // top-max heap -&amp;gt; heapify from down to up func (h *Heap) insert(data int) { if h.</description>
    </item>
    
    <item>
      <title>复杂度分析</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/01_complex_analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/01_complex_analysis/</guid>
      <description>为什么需要复杂度分析#在实际工作中，我们把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析？
首先这种方法叫事后统计法，这种方法的局限性：
 依赖测试环境。代码在不同的环境运行，结果是不同的，比如一个酷睿 i9，和酷睿 i3，很明显 i9 处理速度要快的多。 测试结构受数据规模的影响。测试数据规模太小，测试结果可能无法真实地反应算法的性能。  大 O 时间复杂度表示法#func cal(n int) int { sum := 0 for i := 1; i &amp;lt;= n; ++i { sum = sum + i } return sum } 上面的代码，假设每行代码执行的时间都一样，为 unit_time。那么，第 2、3 行代码分别需要 1 个 unit_time 的执行时间， 第 4、5 行都运行了 n 遍，所以需要 2n * unit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)* unit_time。 可以看出来，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。
func cal2(n int) int { sum := 0 for i := 1; i &amp;lt;= n; i++ { for j := 1; j &amp;lt;= n; j ++ { sum = sum + i } } return sum } 上面的代码，第 2、3 行代码，每行都需要 1 个 unit_time 的执行时间，第 4 行代码循环执行了 n 遍，需要 n * unit_time的执 行时间，第 5,6 行代码循环执行了 n^2 遍，所以需要 2n^2 * unit_time 的执行时间。所以，整段代码总的执行 时间T(n) = (2n^2+n+3)*unit_time。</description>
    </item>
    
    <item>
      <title>字符串匹配算法</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/16_string_match_algo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/16_string_match_algo/</guid>
      <description>字符串匹配算法#字符串匹配算法很常用，比如 js 中的 indexOf 函数，就依赖字符串匹配算法。
BF 算法#BF （Brute Force）算法叫作暴力匹配算法，也叫朴素匹配算法。简单，但是性能差。
字符串匹配算法有两个概念：主串和模式串。比如两个字符串 A 和 B，要在 A 中查找 B，A 就是主串，B 就是模式串。 主串的长度记作 n，模式串的长度记作 m。
BF 算法的思想就是在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。
比如主串是 &amp;ldquo;aaaaa…aaaaaa&amp;rdquo;（省略号表示有很多重复的字符 a），模式串是 &amp;ldquo;aaaaab&amp;rdquo;。每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的 最坏情况时间复杂度是 O(n*m)。
实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。朴素的字符串匹配算法就够用了。
RK 算法#RK 算法（Rabin-Karp）其实就是 BF 算法的升级版。BF 算法需要暴力地对比这 n-m+1 个子串与模式串，但是，每次检查主串与子串是否匹配，需要依 次比对每个字符，所以 BF 算法的时间复杂度就比较高，是 O(n*m)。
RK 算法的思想是通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等（如果不 考虑哈希冲突的问题），那就说明对应的子串和模式串匹配了。但这只是提高了模式串与子串比较的效率。
提高哈希算法计算子串哈希值的效率#怎样设计哈希算法，假设要匹配的字符串的字符集中只包含 K 个字符，可以用一个 K 进制数来表示一个子串，这个 K 进制数转化成十进制数，作为子 串的哈希值。</description>
    </item>
    
    <item>
      <title>排序</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/07_sort/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/07_sort/</guid>
      <description>排序#常用的三类排序算法：
   算法 时间复杂度     冒泡、插入、选择 O(n^2)   快排、归并 O(nlogn)   桶、计数、基数 O(n)    插入排序和冒泡排序的时间复杂度相同，为什么更倾向于使用插入排序算法而不是冒泡排序算法？
分析排序算法#执行效率#排序算法执行效率的分析，可以通过几个方面来衡量：
 最好情况、最坏情况、平均情况时间复杂度 时间复杂度的系数、常数、低阶我们知道，时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但 是实际的软件开发中，排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，就要把 系数、常数、低阶也考虑进来。 基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果在分析排序算法的执行效率的时候， 应该把比较次数和交换（或移动）次数也考虑进去。  内存消耗#算法的内存消耗可以通过空间复杂度来衡量，针对排序算法的空间复杂度，有一个概念，原地排序（Sorted in place），就是特指“空间复杂度” 是 O(1) 的排序算法。冒泡排序，插入排序，选择排序都是原地排序算法。
稳定性#稳定性就是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。
比如一组数据 2，9，3，4，8，3，按照大小排序之后就是 2，3，3，4，8，9。
这组数据里有两个 3。经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法； 如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。
为什么要考察排序算法的稳定性#真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序。 比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望 按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。</description>
    </item>
    
    <item>
      <title>散列表</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/11_hash_table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/11_hash_table/</guid>
      <description>散列表#散列表（Hash Table），也叫哈希表或者 Hash 表。
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。
通过散列函数把元素的键值映射为数组下标，然后将数据存储在数组中对应下标的位置。当按照键值查询元素时，用同样的散列函数，将键 值转化数组下标，从对应的数组下标的位置取数据。
散列函數#散列函数在散列表中起着非常关键的作用。把它定义成 hash(key)，key 表示元素的键值，hash(key) 计算得到一个散列值。
散列函数设计的基本要求：
 散列函数计算得到的散列值是一个非负整，因为数组下标是从 0 开始的。 如果 key1 = key2，那么 hash(key1) == hash(key2) 如果 key1 != key2，那么 hash(key1) != hash(key2)  第三点要注意，在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像的 MD5、SHA、CRC 等哈希算法， 也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。
散列冲突#再好的散列函数也无法避免散列冲突。那该如何解决散列冲突问题？
常用的散列冲突解决方法有两类：
 开放寻址法（open addressing） 链表法（chaining）  开放寻址法#开放寻址法的思想是，如果出现了散列冲突，就重新探测一个空闲位置，将其插入。一种比较简单的探测方法，线性探测（Linear Probing）：
当往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，就从当前位置开始，依次往后查找，看是否有空闲位置， 直到找到为止。
如下图，黄色的色块表示空闲位置，橙色的色块表示已经存储了数据： 图中散列表的大小为 10，在元素 x 插入散列表之前，散列表中已有 6 个元素。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是 这个位置已经有数据了，所以就产生了冲突。于是就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是再从表头 开始找，直到找到空闲位置 2，于是将其插入到这个位置。</description>
    </item>
    
    <item>
      <title>数组</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/02_array/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/02_array/</guid>
      <description>数组#几乎每种语言都有数组这种数据类型，为什么数组的下标都是从 0 开始？
数组的随机访问#数组是一种线性表的数据结构，用一组连续的内存空间，来存储一组相同类型的数据。
线性表
线性表就是数据排程一条线一样的结构。线性表上的数据只有前后两个方向。（链表，栈，队列也是线性表结构）。
二叉树，图，堆等是非线性表结构。非线性表中数据不再是简单的前后关系。
连续的内存空间和相同类型的数据
连续的内存空间和相同类型的数据，正式因为这两个限制，才使数组可以实现随机访问。但是这两个限制也让其他的 操作变得效率低下，比如在数组中插入或删除一个值，为了保证数据的连续，就必须进行数据搬移。
例如：一个长度为 10 的 int 类型的数组 a := new([10]int)。计算机给数组 a[10]，分配了一块连续内存空间 1000～1039， 其中，内存块的首地址为 base_address = 1000。
计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通 过下面的寻址公式，计算出该元素存储的内存地址：
a[i]_address = base_address + i * data_type_sizedata_type_size 表示数组中每个元素的大小。
数组的插入和删除#为什么说数组的插入和删除效率低？
比如一个数组的长度为 n，如果要插入一个元素在 k 位置，时间复杂度是多少?
如果在数组的末尾插入，那么不需要移动数据，在末尾加入元素即可，这时是最好时间复杂度，为 O(1)。如果在数组的开头插入元素，那么所有元素一次向 后移动一位，这时是最坏时间复杂度，为 O(n)。由于在每个位置插入的概率是一样的，那么平均时间复杂度为 (1 + 2 + ...n)/n = O(n)。
删除元素和插入元素差不多。如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况 时间复杂度也为 O(n)。如果数组是无序的，多次删除操作可以集中在一起执行。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组 没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。
如何避免移动数据#如果数组是有序的，那么插入元素就必须移动数据。对于无序的数组，如果要在 k 位置插入元素，为了避免移动数据，可以直接将 k 位置的原数据移动到 数据末尾，把新的元素放在 k 位置。</description>
    </item>
    
    <item>
      <title>栈</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/04_stack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/04_stack/</guid>
      <description>栈#关于“栈”的一个例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取， 不能从中间任意抽出。先进后出，这就是典型的“栈”结构。
栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。
某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，就应该首选“栈”这种数据结构。
栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，叫作顺序栈，用链表实现的栈，叫作链式栈。
对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样了。 当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。
支持动态扩容的顺序栈#基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈 的大小不受限，但要存储 next 指针，内存消耗相对较多。如何基于数组实现一个可以支持动态扩容的栈？
如果要实现一个支持动态扩容的栈，就需要底层依赖一个支持动态扩容的数组。
栈在表达式求值中的应用#编译器如何利用栈来实现表达式求值，比如：34+13*9+44-12/3。
使用两个栈，其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，就直接压入操作数栈；当遇到运算符， 就与运算符栈的栈顶元素进行比较。
如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈 的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。
如何实现浏览器的前进、后退功能#使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当点击前进按 钮时，依次从栈Y中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面 可以点击前进按钮浏览了。</description>
    </item>
    
    <item>
      <title>树</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/13_binary_tree/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/13_binary_tree/</guid>
      <description>树#栈、队列等都是线性表结构，树，一种非线性表结构，这种数据结构比线性表的数据结构要复杂得多。
树的每个元素叫作节点，用来连线相邻节点之间的关系，叫作父子关系。
A 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互 称为兄弟节点。没有父节点的节点叫作根节点，也就是节点 E。没有子节点的节点叫作叶子节点或者叶节点， G、H、I、J、K、L 都是叶子节点。
“树” 的三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。
二叉树#二叉树是最常用的树结构。
二叉树，就是每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点， 有的节点只有左子节点，有的节点只有右子节点。
 编号 2，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。 编号 3 ，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大， 这种二叉树叫作完全二叉树。  二叉树的存储#为什么把最后一层的叶子节点靠左排列的叫完全二叉树？
二叉树的存储方法：
 基于指针或者引用的二叉链式存储法。 基于数组的顺序存储法。  链式存储法#上图是链式存储法，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。只要拎住根节点，就可以通过左右子节点 的指针，把整棵树都串起来。这种存储方式比较常用。大部分二叉树代码都是通过这种结构来实现的。
顺序存储法#上图是顺序存储法，节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储 的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。只要知道根节点存储的位置，就可以把整棵树都串起来。
但是基于数组的顺序存储法，如果是非完全二叉树，其实会浪费比较多的数组存储空间。比如下面途中的例子：
如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要存储额外的左右子节点的指针。 这就是为什么完全二叉树要求最后一层的子节点都靠左的原因。</description>
    </item>
    
    <item>
      <title>索引</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/24_bmore_tree/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/24_bmore_tree/</guid>
      <description>对于存储的需求，无非就是增删改查，并不复杂。但是，一旦存储的数据很多，那性能就成了要关注的重点。 特别是一些跟存储相关的基础系统，比如 MySQL 数据库、消息中间件 RocketMQ 等。这些系统的实现，都离不开索引。
索引可以类比书籍的目录来理解，通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。
设计索引的需求#功能性需求#功能性需求需要考虑的点：
 数据可以分为两类：   结构化数据，比如，MySQL 中的数据； 非结构化数据，比如搜索引擎中网页。非结构化数据一般需要做预处理，提取出查询关键词，对关键词构建索引。  数据是静态还是动态：   静态数据，也就是说，不会有数据的增加、删除、更新操作，在构建索引的时，只需要考虑查询效率就可以了。 动态数据，大部分场景下，都是对动态数据构建索引，不仅要考虑到索引的查询效率，在原始数据更新的同时， 还需要动态地更新索引。设计起来更加复杂。  索引的存储：   索引存储在内存中，查询的速度肯定要比在磁盘中高。但是，如果原始数据量很大的情况下，对应的索引可能也会 很大。内存是有限的，这时就需要考虑存储在磁盘。 一部分存储在内存，一部分存储在磁盘，可以兼顾内存消耗和查询效率。  单值查找还是区间查找：   单值查找，也就是根据查询关键词等于某个值的数据。 区间查找，就是查找关键词处于某个区间值的所有数据。  单关键词查找还是多关键词组合查找：   搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如 “数据结构 AND 算法”。 对于多关键词查询，像 MySQL 这种结构化数据的查询需求，可以实现针对多个关键词的组合，建立索引； 对于像搜索引擎这样的非结构数据的查询需求，可以针对单个关键词构建索引，然后通过集合操作，比如求并集、 求交集等，计算出多个关键词组合的查询结果。  非功能性需求#存储空间#不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非 常有限，一个中间件启动后就占用几个 GB 的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。 但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。
索引的维护成本#索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，也需要 动态的更新索引。而索引的更新势必会影响到增删改操作的性能。</description>
    </item>
    
    <item>
      <title>线性排序</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/08_linear_sort/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/08_linear_sort/</guid>
      <description>线性排序#如何根据年龄给 100 万用户排序？如果使用归并、快排，可以完成功能，但是时间复杂度最低也是 O(nlogn)。有没有更快的排序方法？
桶排序#桶排序，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数 据按照顺序依次取出，组成的序列就是有序的了。
桶排序的时间复杂度#如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度 为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。 当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。
桶排序对数据的要求#桶排序对要排序数据的要求非常苛刻：
 要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要 再进行排序。 数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度 就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。  比如说有 10GB 的订单数据，如果希望按订单金额（假设金额都是正整数）进行排序，但是内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载 到内存中。怎么做？
借助桶排序的处理思想： 先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个 桶里，第一个桶存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且 按照金额范围的大小顺序编号命名（00，01，02…99）。</description>
    </item>
    
    <item>
      <title>贪心算法</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/19_greedy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/19_greedy/</guid>
      <description>贪心、分治、回溯、动态规划是 4 种基本的算法思想。
理解贪心算法#假设有一个可以容纳 100kg 物品的背包，有 5 种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大， 如何选择在背包中装哪些豆子？每种豆子又该装多少？
   种类 总量（kg） 总价值（元）     黄豆 100 100   绿豆 30 90   红豆 60 120   黑豆 20 80   青豆 50 70    这个问题很简单，只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。这个问题的解决思路借助的就是贪心算法。
碰到这类问题，首先要联想到贪心算法：针对一组数据，定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下， 期望值最大。上面的例子，限制值就是重量不能超过 100kg，期望值就是物品的总价值。
贪心算法解决问题的思路，并不总能给出最优解，例如： 在一个有权图中，从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟 当前顶点相连的权最小的边，直到找到顶点 T。按照这种思路，求出的最短路径是 S-&amp;gt;A-&amp;gt;E-&amp;gt;T，路径长度是 1+4+4=9。
但是实际上最短的路径是 S-&amp;gt;B-&amp;gt;D-&amp;gt;T，长度是 2+2+2=6。
在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果第一步从顶点 S 走到顶点 A，那接下来面对的顶 点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便第一步选择最优的走法（边最短），但有可能因为这一步选择，导 致后面每一步的选择都很糟糕。</description>
    </item>
    
    <item>
      <title>跳表</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/10_skip_list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/10_skip_list/</guid>
      <description>跳表#二分查找底层依赖的是数组随机访问的特性，如何用链表实现类似二分查找的算法，就需要对链表稍加改造，这种改造之后的数据结构叫作 跳表（Skip list）。
对于一个单链表，即使链表中存储的数据是有序的，如果要想在其中查找某个数据，也只能从头到尾遍历链表。时间复杂度会是 O(n)。
如果对链表建立一级“索引”，每两个结点提取一个结点到上一级，抽出来的那一级叫作索引或索引层。图中的 down 表示 down 指针，指向下一级结点。
如果现在要查找值为 16 节点。可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，发现下一个结点的值是 17，那要查找 的结点 16 肯定就在这两个结点之间。然后通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，只需要 再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。
加来一层索引之后，查找效率提高了。那如果再加一级索引效率会不会提升更多？
上图中，再来查找 16，只需要遍历 6 个结点，需要遍历的结点数量又减少了。
上图中，一个包含 64 个结点的链表，按照前面思路，建立了五级索引。原来没有索引的时候，查找 62 需要遍历 62 个结点，现在 只需要遍历 11 个结点，速度提高了很多。所以，当链表的长度 n 比较大时，比如 1000、10000 的时候，在构建索引之后，查找效 率的提升就会非常明显。
链表加多级索引的结构，就是跳表。
跳表的时间复杂度#如果链表里有 n 个结点，按照前面的思路，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二 级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第k级索引的结点个数是第 k-1 级索引的结 点个数的 1/2，那第 k 级索引结点的个数就是 n/(2^k)。</description>
    </item>
    
    <item>
      <title>递归</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/06_recursion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/06_recursion/</guid>
      <description>递归#比如在影院，你不知道你现在坐在第几排，就可以问前一排的人他在第几排，如果前面的人也不知道，再问前一排人，直到问道第一排。再把这个数字 一排一排传回来，就知道在第几排了。这个过程就是递归。去的过程是“递”，回来的过程是“归”。
递归需要的三个条件# 一个问题的解可以分解为几个子问题的解何为子问题？子问题就是数据规模更小的问题。 比如你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样。 比如你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。 存在递归终止条件。 例子中第一排的人不需要再继续询问任何人，就知道自己在哪一排，这就是递归的终止条件。  递归代码最关键的是写出递推公式，找到终止条件。
堆栈溢出#函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟 机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。
比如前面电影院的例子，如果我们将系统栈或者 JVM 堆栈大小设置为 1KB，在求解 f(19999) 时便会出现如下堆栈报错： Exception in thread &amp;quot;main&amp;quot; java.lang.StackOverflowError
如何避免出现堆栈溢出#可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了， 直接返回报错。
重复计算#使用递归时还会出现重复计算的问题。可以通过一个数据结构（比如散列表）来保存已经求解过的递归函数。</description>
    </item>
    
    <item>
      <title>链表</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/03_link_list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/03_link_list/</guid>
      <description>链表#链表也是一种基础的数据结构。常见的链表结构有：单链表，双向链表，循环链表。
单链表#链表通过指针将一组零散的内存块串联在一起。其中内存块称为链表的结点。为了将所有的结点串起来，每个链表的结点除了存储数据之外， 还需要记录链上的下一个结点的地址。这个记录下个结点地址的指针叫作后继指针 next。
其中有两个结点是比较特殊的，它们分别是头结点和尾结点。其中，头结点用来记录链表的基地址。有了它，就可以遍历得到整条链表。而尾结点 的指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。
在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一 个数据是非常快速的。链表的插入和删除操作，只需要考虑相邻结点的指针改变，时间复杂度是 O(1)。
虽然插入和删除变得高效了，但是这也导致链表的随机访问没有数组效率高，因为无法像数组那样直接通过寻址公式计算出下标对应的内存地址，必须 根据后继指针来遍历每一个节点。所以链表的随机减访问时间复杂度为 O(n)。
循环链表#循环链表只是一种特殊的单链表，唯一的不同就是循环链表的尾节点的指针不指向空地址 NULL，而是头节点。
循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如约瑟夫问题。
双向链表#单向链表只有一个方向，而双向链表，有两个方向。每个结点不止有一个后继指针 next 指向后面的结点，还有一个 前驱指针 prev 指向前面的结点。
双向链表比单链表需要额外的空间来存储前驱指针，因此同样的数据要比单链表占用更多的内存空间。
双向链表的优势#双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。
  删除结点中“值等于某个给定值”的结点 对于这种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点， 然后再通过我前面讲的指针操作将其删除。尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。 根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。
  删除给定指针指向的结点 这种情况，加入我们已经找到了要删除的结点 q，得到了 q 节点的指针，但是要删除删除这个结点 q 还需要知道其前驱结点，而单链表并不支持直接获取 前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&amp;gt;next=q，说明 p 是 q 的前驱结点。但是对于双向链表来说，因为 双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对这种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表 只需要 O(1) 的时间复杂度。
  对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系， 决定是往前还是往后查找，所以平均只需要查找一半的数据。</description>
    </item>
    
    <item>
      <title>队列</title>
      <link>http://shipengqi.github.io/algorithm-learn/docs/05_queue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shipengqi.github.io/algorithm-learn/docs/05_queue/</guid>
      <description>队列#队列，先进先出，像排队买票一样。队列和栈很像，也是一种操作受限的线性表数据结构，有两个基本操作：
 入队（enqueue），放一个数据到队列尾部。 出队（dequeue），从队列头部取一个元素。  顺序队列和链式队列#顺序队列#跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。
type ArrayQueue struct { items []string n int // 数组 size 	head int // 队列头下标 	tail int // 队列尾下标 } func (a *ArrayQueue) NewArrayQueue(capacity int) { a.items = []string{} a.n = capacity a.head = 0 a.tail = 0 } // 入队 func (a *ArrayQueue) Enqueue(item string) bool { if a.tail == a.n { // 队列已满 	return false } a.</description>
    </item>
    
  </channel>
</rss>